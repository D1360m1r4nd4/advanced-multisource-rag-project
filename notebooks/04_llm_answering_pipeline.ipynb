{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580539c4",
   "metadata": {},
   "source": [
    "# Notebook 04 LLM Answering Pipeline\n",
    "This section is responsible for:\n",
    "\n",
    "* Running a local, open-source LLM\n",
    "* Constructing the final prompt (query + context)\n",
    "* Generating an answer\n",
    "* Attaching citations\n",
    "* Returning a polished response\n",
    "\n",
    "Implementing open-source software, we will use:\n",
    "\n",
    "Ollama (best, simplest, FREE local LLM runner)\n",
    "\n",
    "Model: llama3 (this can changed eventually in order to improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5a0c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded context_for_llm OK.\n"
     ]
    }
   ],
   "source": [
    "# Had issues keeping same directories and variables across notebooks. \n",
    "# This should fix it.\n",
    "\n",
    "from config import CONTEXT_FILE\n",
    "import json\n",
    "\n",
    "if not CONTEXT_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"context_for_llm not found. Expected at: {CONTEXT_FILE}\\n\"\n",
    "        \"Run Notebook 03 first.\"\n",
    "    )\n",
    "\n",
    "with open(CONTEXT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    context_for_llm = json.load(f)\n",
    "\n",
    "print(\"Loaded context_for_llm OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e768f8b",
   "metadata": {},
   "source": [
    "# 1 Import and Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79023bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback HTTP Ollama client (requests).\n"
     ]
    }
   ],
   "source": [
    "# Notebook 04 — Cell 1\n",
    "# Imports, configuration, and Ollama client setup (robust fallback)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from typing import Dict, Optional, Any\n",
    "\n",
    "# Use the official ollama client (if installed).\n",
    "# If not, fall back to a safe HTTP-based client using `requests`.\n",
    "llm = None\n",
    "\n",
    "try:\n",
    "    # Preferred: official Python client\n",
    "    from ollama import Client as OllamaClient\n",
    "    # If the ollama client exists, create a client bound to the local daemon\n",
    "    llm = OllamaClient(host=os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "    print(\"Official Ollama client loaded.\")\n",
    "except Exception:\n",
    "    # Fallback: HTTP requests-based client\n",
    "    try:\n",
    "        import requests\n",
    "        from requests.exceptions import HTTPError, RequestException\n",
    "\n",
    "        class SimpleOllamaClient:\n",
    "            \"\"\"Lightweight HTTP Ollama client that normalizes responses.\"\"\"\n",
    "\n",
    "            def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "                self.host = host.rstrip(\"/\")\n",
    "\n",
    "            def _normalize(self, data: Any) -> Dict:\n",
    "                # Normalize a variety of response shapes to:\n",
    "                # {\"message\": {\"content\": \"<text>\"}}\n",
    "                if isinstance(data, dict):\n",
    "                    if \"response\" in data and isinstance(data[\"response\"], str):\n",
    "                        return {\"message\": {\"content\": data[\"response\"]}}\n",
    "                    if \"text\" in data and isinstance(data[\"text\"], str):\n",
    "                        return {\"message\": {\"content\": data[\"text\"]}}\n",
    "                    if \"message\" in data and isinstance(data[\"message\"], dict) and \"content\" in data[\"message\"]:\n",
    "                        return {\"message\": {\"content\": data[\"message\"][\"content\"]}}\n",
    "                    if \"choices\" in data and isinstance(data[\"choices\"], list) and len(data[\"choices\"]) > 0:\n",
    "                        first = data[\"choices\"][0]\n",
    "                        # handle openai-style or other shapes\n",
    "                        if isinstance(first, dict):\n",
    "                            if \"text\" in first:\n",
    "                                return {\"message\": {\"content\": first[\"text\"]}}\n",
    "                            if \"message\" in first and isinstance(first[\"message\"], dict) and \"content\" in first[\"message\"]:\n",
    "                                return {\"message\": {\"content\": first[\"message\"][\"content\"]}}\n",
    "                # Fallback: string conversion\n",
    "                return {\"message\": {\"content\": str(data)}}\n",
    "\n",
    "            def chat(self, model: str, messages: list, timeout: int = 60) -> Dict:\n",
    "                \"\"\"\n",
    "                Sends a consolidated prompt to Ollama HTTP /api/generate endpoint.\n",
    "                Expects `messages` as a list of {\"role\": \"...\", \"content\": \"...\"}; we merge user messages.\n",
    "                \"\"\"\n",
    "                prompt_text = \"\\n\".join([m.get(\"content\", \"\") for m in messages if m.get(\"role\") == \"user\"]).strip()\n",
    "                url = f\"{self.host}/api/generate\"\n",
    "                payload = {\"model\": model, \"prompt\": prompt_text, \"stream\": False}\n",
    "                try:\n",
    "                    resp = requests.post(url, json=payload, timeout=timeout)\n",
    "                    resp.raise_for_status()\n",
    "                except HTTPError as he:\n",
    "                    status = getattr(he.response, \"status_code\", None)\n",
    "                    if status == 404:\n",
    "                        raise RuntimeError(f\"Ollama HTTP endpoint not found (404). Is Ollama running at {self.host}?\") from he\n",
    "                    raise\n",
    "                except RequestException as re:\n",
    "                    raise RuntimeError(f\"Failed to reach Ollama at {self.host}: {re}\") from re\n",
    "\n",
    "                # Parse JSON when possible\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except ValueError:\n",
    "                    # Non-json, return as text\n",
    "                    return {\"message\": {\"content\": resp.text}}\n",
    "                return self._normalize(data)\n",
    "\n",
    "        llm = SimpleOllamaClient(host=os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "        print(\"Using fallback HTTP Ollama client (requests).\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not initialize Ollama client or HTTP fallback.\")\n",
    "        print(\"Install `ollama` python package or `requests`, and ensure Ollama daemon is running.\")\n",
    "        class _StubLLM:\n",
    "            def chat(self, *args, **kwargs):\n",
    "                raise RuntimeError(\"No Ollama client available. Install `ollama` or start Ollama daemon.\")\n",
    "        llm = _StubLLM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762712c3",
   "metadata": {},
   "source": [
    "# 2 Prompt Template and Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3155f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 04 — Cell 2\n",
    "# Prompt template + builder for RAG. Keep the prompt strict to avoid hallucination.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "def build_rag_prompt(query: str, context_data: Dict, instructions: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Build the final prompt for the LLM using the assembled `context_data`.\n",
    "    Expects context_data keys: merged_context (string), citations (list).\n",
    "    \"\"\"\n",
    "\n",
    "    context_block = context_data.get(\"merged_context\", \"\").strip()\n",
    "    # Short helper to format a compact citations list (not strictly required by the LLM)\n",
    "    citation_lines = []\n",
    "    for c in context_data.get(\"citations\", []):\n",
    "        src = c.get(\"source\", \"unknown\")\n",
    "        preview = c.get(\"chunk_preview\", \"\")[:120]\n",
    "        citation_lines.append(f\"- {src}: {preview}\")\n",
    "\n",
    "    citation_text = \"\\n\".join(citation_lines)\n",
    "\n",
    "    # Base instructions\n",
    "    base_instructions = \"\"\"\n",
    "You are an enterprise-grade retrieval assistant. Answer using ONLY the provided context.\n",
    "Do NOT hallucinate. If the required information is not present in the context, respond:\n",
    "\"The answer is not available in the provided context.\"\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Cite sources inline for any factual claim using the bracket format: [source]\n",
    "  Example: \"6G targets ultra-low latency [./data/6g-paper.pdf]\"\n",
    "- Keep answers concise and reference the citation location when possible.\n",
    "\"\"\"\n",
    "\n",
    "    if instructions:\n",
    "        base_instructions = base_instructions + \"\\n\" + instructions\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{base_instructions}\n",
    "\n",
    "CONTEXT:\n",
    "--------\n",
    "{context_block}\n",
    "--------\n",
    "\n",
    "CITATIONS:\n",
    "{citation_text}\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "FINAL ANSWER (include inline citations where relevant):\n",
    "\"\"\".strip()\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c9dc2",
   "metadata": {},
   "source": [
    "# 3 Answer Generation Using Local LLM (Ollama + Llama3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f772ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 04 — Cell 3\n",
    "# Core LLM invocation wrapper. Uses llm.chat() and normalizes outputs.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def generate_llm_answer(query: str, context_for_llm: Dict, model: Optional[str] = None, timeout: int = 60) -> Dict:\n",
    "    \"\"\"\n",
    "    Build prompt from context_for_llm and call the configured local LLM.\n",
    "    Returns a dictionary with fields:\n",
    "      - 'answer' (str) : model textual output\n",
    "      - 'meta' (dict)  : underlying raw response (if available)\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:latest\")\n",
    "\n",
    "    prompt = build_rag_prompt(query, context_for_llm)\n",
    "\n",
    "    # Call the client. The client returns a normalized structure (we implemented normalization in the SimpleOllamaClient)\n",
    "    try:\n",
    "        raw_resp = llm.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], timeout=timeout)\n",
    "    except TypeError:\n",
    "        # Some clients may not accept timeout param; try again without it\n",
    "        raw_resp = llm.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\") from e\n",
    "\n",
    "    # Normalize the output to extract a string answer.\n",
    "    answer_text = None\n",
    "    # raw_resp could already be a dict normalized by our HTTP client\n",
    "    if isinstance(raw_resp, dict):\n",
    "        # common pattern: {\"message\": {\"content\": \"...\"}}\n",
    "        if \"message\" in raw_resp and isinstance(raw_resp[\"message\"], dict) and \"content\" in raw_resp[\"message\"]:\n",
    "            answer_text = raw_resp[\"message\"][\"content\"]\n",
    "        elif \"response\" in raw_resp and isinstance(raw_resp[\"response\"], str):\n",
    "            answer_text = raw_resp[\"response\"]\n",
    "        elif \"text\" in raw_resp and isinstance(raw_resp[\"text\"], str):\n",
    "            answer_text = raw_resp[\"text\"]\n",
    "        elif \"choices\" in raw_resp and isinstance(raw_resp[\"choices\"], list) and len(raw_resp[\"choices\"]) > 0:\n",
    "            first = raw_resp[\"choices\"][0]\n",
    "            if isinstance(first, dict):\n",
    "                answer_text = first.get(\"text\") or (first.get(\"message\", {}).get(\"content\") if isinstance(first.get(\"message\"), dict) else None)\n",
    "\n",
    "    # Last fallback: stringify whatever was returned\n",
    "    if answer_text is None:\n",
    "        try:\n",
    "            answer_text = str(raw_resp)\n",
    "        except Exception:\n",
    "            answer_text = \"<no textual output>\"\n",
    "\n",
    "    return {\"answer\": answer_text, \"meta\": raw_resp}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516298f",
   "metadata": {},
   "source": [
    "# 4 Final Output Formatting - Answer + Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9431887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 04 — Cell 4\n",
    "# Formatting final output (print + structured return)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def format_final_output(answer_obj: Dict, context_for_llm: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Prints a human-friendly final answer + citations and returns a structured dict:\n",
    "      { \"answer\": str, \"citations\": [...], \"raw_context\": str }\n",
    "    The input answer_obj should be the dict returned by generate_llm_answer().\n",
    "    \"\"\"\n",
    "    answer_text = answer_obj.get(\"answer\", \"\")\n",
    "    raw_meta = answer_obj.get(\"meta\", {})\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(\"=\" * 40 + \"\\n\")\n",
    "    print(textwrap.fill(answer_text, width=100))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"CITATIONS\")\n",
    "    print(\"=\" * 40 + \"\\n\")\n",
    "    citations = []\n",
    "    for c in context_for_llm.get(\"citations\", []):\n",
    "        # Defensive read with defaults\n",
    "        src = c.get(\"source\", \"unknown\")\n",
    "        preview = c.get(\"chunk_preview\", \"\") or c.get(\"text\", \"\")[:120]\n",
    "        rerank_score = c.get(\"rerank_score\")\n",
    "        score_str = f\"{rerank_score:.4f}\" if isinstance(rerank_score, (int, float)) else \"n/a\"\n",
    "        print(f\"- Source: {src}\")\n",
    "        print(f\"  Preview: {preview}\")\n",
    "        print(f\"  Rerank score: {score_str}\\n\")\n",
    "        citations.append({\"source\": src, \"preview\": preview, \"rerank_score\": rerank_score})\n",
    "\n",
    "    # Return structured object for programmatic use (APIs, UI)\n",
    "    return {\n",
    "        \"answer\": answer_text,\n",
    "        \"citations\": citations,\n",
    "        \"raw_context\": context_for_llm.get(\"merged_context\", \"\"),\n",
    "        \"llm_meta\": raw_meta\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8dae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running end-to-end test with current context_for_llm...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "LLM call failed: Failed to reach Ollama at http://localhost:11434: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mSimpleOllamaClient.chat\u001b[39m\u001b[34m(self, model, messages, timeout)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     resp.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/advanced-multisource-rag-project/.venv/lib/python3.12/site-packages/requests/adapters.py:690\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate_llm_answer\u001b[39m\u001b[34m(query, context_for_llm, model, timeout)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     raw_resp = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Some clients may not accept timeout param; try again without it\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mSimpleOllamaClient.chat\u001b[39m\u001b[34m(self, model, messages, timeout)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RequestException \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reach Ollama at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Parse JSON when possible\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to reach Ollama at http://localhost:11434: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning end-to-end test with current context_for_llm...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     resp_obj = \u001b[43mgenerate_llm_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_for_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     final = format_final_output(resp_obj, context_for_llm)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# `final` is a structured dict you can use for downstream tasks\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mgenerate_llm_answer\u001b[39m\u001b[34m(query, context_for_llm, model, timeout)\u001b[39m\n\u001b[32m     22\u001b[39m     raw_resp = llm.chat(model=model, messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}])\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM call failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Normalize the output to extract a string answer.\u001b[39;00m\n\u001b[32m     27\u001b[39m answer_text = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: LLM call failed: Failed to reach Ollama at http://localhost:11434: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)"
     ]
    }
   ],
   "source": [
    "# Notebook 04 — Cell 5\n",
    "# Test cell — run end-to-end assuming `context_for_llm` exists (from Notebook 03).\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Replace this sample query with any user question\n",
    "test_query = \"What does 6G offer?\"\n",
    "\n",
    "if \"context_for_llm\" not in globals():\n",
    "    print(\"Context_for_llm not found in kernel. Run Notebook 03 first to produce it.\")\n",
    "else:\n",
    "    print(\"Running end-to-end test with current context_for_llm...\")\n",
    "    resp_obj = generate_llm_answer(test_query, context_for_llm, model=None)\n",
    "    final = format_final_output(resp_obj, context_for_llm)\n",
    "    # `final` is a structured dict you can use for downstream tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c43b813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context keys: dict_keys(['merged_context', 'citations', 'raw_chunks'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Context keys:\", context_for_llm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcece305",
   "metadata": {},
   "source": [
    "# ---- Ollama HTTP diagnostic (run this cell to test your local Ollama docker)\n",
    "import os\n",
    "print(\"--- Ollama HTTP Diagnostic ---\")\n",
    "host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "model = os.getenv('OLLAMA_MODEL', 'llama3.1:latest')\n",
    "print(f'Using host={host} model={model}')\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print('requests version:', requests.__version__)\n",
    "except Exception as e:\n",
    "    print('requests not available in this kernel:', e)\n",
    "\n",
    "# Quick GET checks for common endpoints\n",
    "for path in ['', '/api', '/api/models', '/api/generate']:\n",
    "    url = host.rstrip('/') + path\n",
    "    try:\n",
    "        r = requests.get(url, timeout=4)\n",
    "        print(f'GET {path} ->', r.status_code)\n",
    "    except Exception as e:\n",
    "        print(f'GET {path} error:', e)\n",
    "\n",
    "# Try a small generate POST to /api/generate\n",
    "payload = {'model': model, 'prompt': 'Say hello and identify the model used.', 'stream': False}\n",
    "try:\n",
    "    gen_url = host.rstrip('/') + '/api/generate'\n",
    "    r = requests.post(gen_url, json=payload, timeout=20)\n",
    "    print('POST /api/generate ->', r.status_code)\n",
    "    text = r.text\n",
    "    print('Response (truncated):', text[:800])\n",
    "    try:\n",
    "        j = r.json()\n",
    "        print('JSON keys:', list(j.keys()))\n",
    "        # If llm object exists with _normalize, try to normalize and show result\n",
    "        if 'llm' in globals() and hasattr(llm, '_normalize'):\n",
    "            try:\n",
    "                print('Normalized sample:', llm._normalize(j))\n",
    "            except Exception as e:\n",
    "                print('Normalization error:', e)\n",
    "        else:\n",
    "            # Try simple normalization heuristics\n",
    "            if isinstance(j, dict):\n",
    "                if 'message' in j:\n",
    "                    print('message:', j.get('message'))\n",
    "                elif 'text' in j:\n",
    "                    print('text:', j.get('text'))\n",
    "                elif 'choices' in j and isinstance(j['choices'], list) and len(j['choices'])>0:\n",
    "                    print('choices[0]:', j['choices'][0])\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('POST /api/generate error:', e)\n",
    "\n",
    "print('Diagnostic complete. If you see 404 from POST, verify the Ollama daemon and model name (try OLLAMA_MODEL=llama3.1:latest).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
