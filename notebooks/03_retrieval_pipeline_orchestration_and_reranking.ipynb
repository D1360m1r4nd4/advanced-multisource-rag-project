{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b9a18",
   "metadata": {},
   "source": [
    "# Notebook 03 Retrieval Pipeline Orchestration + Reranking\n",
    "\n",
    "**Goal**\n",
    "Build a hybrid retrieval system that:\n",
    "\n",
    "1. Pulls candidate results from your Qdrant vector store (semantic retrieval).\n",
    "\n",
    "2. Optionally augments those with graph-based or metadata filters (hierarchical or relational layer).\n",
    "\n",
    "3. Applies cross-encoder reranking to refine relevance before passing context to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d494c",
   "metadata": {},
   "source": [
    "# 1: Setup & Connections\n",
    "\n",
    "* Import required libraries.\n",
    "* Load/connect to Qdrant (vector DB) and Memgraph (graph DB).\n",
    "* Load the embedding model (for encoding queries) and the cross-encoder reranker (for re-scoring * candidate documents).\n",
    "* Define some configuration variables (collection name, top_k defaults).\n",
    "* Print short status messages so you know everything connected/loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb589d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Qdrant at http://localhost:6333. Collections: ['enterprise_docs']\n",
      "Connected to Memgraph at localhost:7687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0110e753ddf4d19ad4f3e784d3c41b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BGE-M3 embedder: BAAI/bge-m3\n",
      "Loaded reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "--- Summary ---\n",
      "Qdrant client: OK\n",
      "Memgraph client: OK\n",
      "Embedder: OK (BGE-M3)\n",
      "Reranker: OK (cross-encoder/ms-marco-MiniLM-L-6-v2)\n",
      "Collection in use: enterprise_docs\n",
      "Retrieval settings: DEFAULT_TOP_K=50, FINAL_TOP_K=5\n"
     ]
    }
   ],
   "source": [
    "# Before running this notebook, ensure Notebooks 01–02 are done\n",
    "# Qdrant + Memgraph must be running\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "# Memgraph\n",
    "from gqlalchemy import Memgraph\n",
    "\n",
    "# NEW: BGE-M3 embedder\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# Cross-encoder for reranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# ----- Configuration -----\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "MEMGRAPH_HOST = os.getenv(\"MEMGRAPH_HOST\", \"localhost\")\n",
    "MEMGRAPH_PORT = int(os.getenv(\"MEMGRAPH_PORT\", 7687))\n",
    "\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION\", \"enterprise_docs\")\n",
    "\n",
    "DEFAULT_TOP_K = 50   # initial candidates\n",
    "FINAL_TOP_K = 5      # final reranked results\n",
    "\n",
    "\n",
    "# ----- Connect to Qdrant -----\n",
    "\n",
    "try:\n",
    "    qdrant_client = QdrantClient(url=QDRANT_URL)\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"Connected to Qdrant at {QDRANT_URL}. Collections: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    qdrant_client = None\n",
    "    print(\"Could not connect to Qdrant:\", e)\n",
    "\n",
    "\n",
    "# ----- Connect to Memgraph -----\n",
    "\n",
    "try:\n",
    "    memgraph = Memgraph(host=MEMGRAPH_HOST, port=MEMGRAPH_PORT)\n",
    "    test = list(memgraph.execute_and_fetch(\"RETURN 1 AS ok\"))\n",
    "    print(f\"Connected to Memgraph at {MEMGRAPH_HOST}:{MEMGRAPH_PORT}\")\n",
    "except Exception as e:\n",
    "    memgraph = None\n",
    "    print(\"Could not connect to Memgraph:\", e)\n",
    "\n",
    "\n",
    "# ----- Load BGE-M3 Embedder -----\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "\n",
    "try:\n",
    "    embedder = BGEM3FlagModel(EMBEDDING_MODEL_NAME, use_fp16=False)\n",
    "    print(f\"Loaded BGE-M3 embedder: {EMBEDDING_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    embedder = None\n",
    "    print(\"Could not load BGE-M3 embedder:\", e)\n",
    "\n",
    "\n",
    "# ----- Query embedding helper (BGE-M3) -----\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    \"\"\"Return normalized 1024-d query embedding using BGE-M3.\"\"\"\n",
    "    out = embedder.encode(\n",
    "        text,\n",
    "        max_length=8192,\n",
    "        return_dense=True,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    vec = out[\"dense_vecs\"]\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0:\n",
    "        vec = vec / norm\n",
    "    return vec.tolist()\n",
    "\n",
    "\n",
    "# ----- Load reranker -----\n",
    "\n",
    "RERANKER_MODEL_NAME = os.getenv(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "try:\n",
    "    reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
    "    print(f\"Loaded reranker: {RERANKER_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    reranker = None\n",
    "    print(\"Could not load reranker:\", e)\n",
    "\n",
    "\n",
    "# ----- Summary -----\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Qdrant client: {'OK' if qdrant_client else 'MISSING'}\")\n",
    "print(f\"Memgraph client: {'OK' if memgraph else 'MISSING'}\")\n",
    "print(f\"Embedder: {'OK' if embedder else 'MISSING'} (BGE-M3)\")\n",
    "print(f\"Reranker: {'OK' if reranker else 'MISSING'} ({RERANKER_MODEL_NAME})\")\n",
    "print(f\"Collection in use: {COLLECTION_NAME}\")\n",
    "print(f\"Retrieval settings: DEFAULT_TOP_K={DEFAULT_TOP_K}, FINAL_TOP_K={FINAL_TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd2eb7",
   "metadata": {},
   "source": [
    "# 2 Semantic Retrieval from Qdrant\n",
    "\n",
    "Here we will:\n",
    "1. Take a natural language query.\n",
    "2. Encode it into a vector using your embedding model.\n",
    "3. Search for the most semantically similar chunks (documents) in Qdrant.\n",
    "4. Return and display the top K results, with metadata for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d02be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results for query: 'What does 6G offer?'\n",
      "\n",
      "Result 1: (score=0.6629)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit...\n",
      "\n",
      "Result 2: (score=0.6524)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis...\n",
      "\n",
      "Result 3: (score=0.6511)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc...\n",
      "\n",
      "Result 4: (score=0.6384)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n...\n",
      "\n",
      "Result 5: (score=0.6373)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . M. Tatipamula is with Ericsson Silicon Valley, Santa Clara, CA 95054, USA (e-mail: mallik.tatipamula@ericsson.com). 3202 nu J 6 ]IN.sc[ 2 v 23800.6032:vi Xra 1 Towards 6 G: evolution in the Making atipamula, and Muhammad Ali Imran systems toward their limits within 10 years of their launch and cal...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_retrieval(query: str, top_k: int = 5, collection_name: str = COLLECTION_NAME):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant documents from Qdrant using BGE-M3 embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Embed query using BGE-M3 ---\n",
    "    query_vector = embed_query(query)   # returns normalized 1024-d list\n",
    "\n",
    "    # --- 2) Qdrant vector search using new API ---\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "\n",
    "    # --- 3) Format results ---\n",
    "    formatted = []\n",
    "    for point in results.points:\n",
    "        formatted.append({\n",
    "            \"score\": point.score,\n",
    "            \"text\": point.payload.get(\"text\", \"\"),\n",
    "            \"source\": point.payload.get(\"source\", \"\"),\n",
    "            \"chunk_id\": point.payload.get(\"chunk_id\", \"\"),\n",
    "            \"metadata\": point.payload\n",
    "        })\n",
    "\n",
    "    return formatted\n",
    "\n",
    "# ===========================================\n",
    "# Example Query — try one from your dataset if available\n",
    "# Replace this with a question relevant to your data.\n",
    "sample_query = \"What does 6G offer?\"\n",
    "\n",
    "# Run the retrieval function\n",
    "retrieved_docs = semantic_retrieval(sample_query, top_k=5)\n",
    "\n",
    "# Display retrieved documents\n",
    "print(f\"\\nTop {len(retrieved_docs)} results for query: '{sample_query}'\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i}: (score={doc['score']:.4f})\")\n",
    "    print(f\"Source: {doc['source']} | Page: {doc.get('page')}\")\n",
    "    print(f\"Snippet: {doc['text'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038b188",
   "metadata": {},
   "source": [
    "# 3 Hybrid Retrieval (Vector + Graph)\n",
    "This next cell implements a hybrid retrieval layer that merges: \n",
    "1. **Semantic relevance**\n",
    "2. **Graph-based context**\n",
    "\n",
    "This hybrid approach improves entreprise knowledge retrieval by connecting related entities instead of relying exclusively on text similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c12fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 hybrid candidates for: What does 6G offer?\n",
      "\n",
      "Result 1: [vector] score=0.6629 source=./data/test.pdf\n",
      ". The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit \n",
      "--------------------------------------------------------------------------------\n",
      "Result 2: [vector] score=0.6524 source=./data/test.pdf\n",
      ". II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis \n",
      "--------------------------------------------------------------------------------\n",
      "Result 3: [vector] score=0.6511 source=./data/test.pdf\n",
      ". In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc \n",
      "--------------------------------------------------------------------------------\n",
      "Result 4: [vector] score=0.6384 source=./data/test.pdf\n",
      ". This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n \n",
      "--------------------------------------------------------------------------------\n",
      "Result 5: [vector] score=0.6373 source=./data/test.pdf\n",
      ". M. Tatipamula is with Ericsson Silicon Valley, Santa Clara, CA 95054, USA (e-mail: mallik.tatipamula@ericsson.com). 3202 nu J 6 ]IN.sc[ 2 v 23800.6032:vi Xra 1 Towards 6 G: evolution in the Making atipamula, and Muhammad Ali Imran systems toward their limits within 10 years of their launch and cal \n",
      "--------------------------------------------------------------------------------\n",
      "Result 6: [vector] score=0.6184 source=./data/test.pdf\n",
      ". III. DRIVING TRENDS AND USE CASES (UCS) The prospected 6 G promise stems from four main driving UCs, which both lay the foundation for numerous applications and set the technical requirements for future communication systems. These UCs are shown in Fig. 1 and are detailed in this section. Internet \n",
      "--------------------------------------------------------------------------------\n",
      "Result 7: [vector] score=0.6058 source=./data/test.pdf\n",
      ". and diversified services settings, transparency and affordance, collaboration and communication, access and privacy, and a range of interaction types ranging from real-time interactive to highly asynchronous [2]. Finally, Fig. 5 shows several stepping stones, linking the Io Mus T journey with the  \n",
      "--------------------------------------------------------------------------------\n",
      "Result 8: [vector] score=0.6028 source=./data/test.pdf\n",
      ". As such, a fundamental enabling technology is network adaptability, which refers to the idea of enabling rapid network deployments and the fast introduction of new services. This comprises dynamic network deployments, which includes ad-hoc or temporal deployments and mobile and non-terrestrial net \n",
      "--------------------------------------------------------------------------------\n",
      "Result 9: [vector] score=0.5954 source=./data/test.pdf\n",
      ". To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy lean systems and network sharing; 2) responsible ICT, that ensures transparency and traceability, secures human rights, and guarantees children online protection; 3) ICT for inclu \n",
      "--------------------------------------------------------------------------------\n",
      "Result 10: [vector] score=0.5920 source=./data/test.pdf\n",
      "The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstract—While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (Io T), the unprecedented strain on communication networks put by t \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fixed semantic_retrieval + hybrid_retrieval_pipeline\n",
    "# Uses: BGE-M3 embedder and new Qdrant client API (query_points)\n",
    "# -------------------------\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def semantic_retrieval(query: str, top_k: int = 20, collection_name: str = COLLECTION_NAME) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k vector matches from Qdrant using BGE-M3 query embedding.\n",
    "    Uses qdrant_client.query_points(...) (new client API).\n",
    "    Returns a list of dicts with keys: score, text, source, chunk_id, source_type.\n",
    "    \"\"\"\n",
    "    if qdrant_client is None:\n",
    "        print(\"⚠️ Qdrant client not connected — skipping semantic retrieval.\")\n",
    "        return []\n",
    "\n",
    "    if embedder is None:\n",
    "        print(\"⚠️ Embedder not loaded — cannot run semantic retrieval.\")\n",
    "        return []\n",
    "\n",
    "    # 1) Embed the query consistently with how chunks were embedded\n",
    "    out = embedder.encode(\n",
    "        query,\n",
    "        max_length=8192,\n",
    "        return_dense=True,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "\n",
    "    qvec = out[\"dense_vecs\"]\n",
    "    # ensure numpy array -> normalize -> list\n",
    "    qvec = np.asarray(qvec, dtype=float)\n",
    "    norm = np.linalg.norm(qvec)\n",
    "    if norm > 0:\n",
    "        qvec = (qvec / norm).tolist()\n",
    "    else:\n",
    "        qvec = qvec.tolist()\n",
    "\n",
    "    # 2) Use the new Qdrant API to query points\n",
    "    # Note: some qdrant-client versions expose query_points(...) which returns an object with .points\n",
    "    try:\n",
    "        resp = qdrant_client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=qvec,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "    except AttributeError:\n",
    "        # Fallback for slightly different client API names\n",
    "        resp = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=qvec,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "    # 3) Normalize/form the response\n",
    "    formatted: List[Dict[str, Any]] = []\n",
    "    # If resp has .points (new API)\n",
    "    points = getattr(resp, \"points\", None)\n",
    "    if points is None:\n",
    "        # older return type: resp might already be a list\n",
    "        points = resp\n",
    "\n",
    "    for p in points:\n",
    "        # p may be a Point object or a dict depending on client version\n",
    "        score = getattr(p, \"score\", None)\n",
    "        payload = getattr(p, \"payload\", None) or (p.get(\"payload\") if isinstance(p, dict) else None)\n",
    "\n",
    "        formatted.append({\n",
    "            \"score\": float(score) if score is not None else 0.0,\n",
    "            \"text\": payload.get(\"text\", \"\") if payload else \"\",\n",
    "            \"source\": payload.get(\"source\", \"\") if payload else \"\",\n",
    "            \"chunk_id\": payload.get(\"chunk_id\", \"\") if payload else \"\",\n",
    "            \"metadata\": payload or {},\n",
    "            \"source_type\": \"vector\"\n",
    "        })\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def graph_retrieval(query: str, memgraph_conn: Memgraph, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Safe) graph retrieval to fetch chunks from Memgraph matching the query.\n",
    "    Keeps same output schema as semantic_retrieval.\n",
    "    \"\"\"\n",
    "    if memgraph_conn is None:\n",
    "        return []\n",
    "\n",
    "    cypher_query = f\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE toLower(c.text) CONTAINS toLower(\"{query}\")\n",
    "    RETURN c.text AS text, c.source AS source, c.chunk_id AS chunk_id\n",
    "    LIMIT {limit};\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        rows = list(memgraph_conn.execute_and_fetch(cypher_query))\n",
    "    except Exception as e:\n",
    "        print(\"Graph retrieval failed:\", e)\n",
    "        return []\n",
    "\n",
    "    formatted = []\n",
    "    for r in rows:\n",
    "        formatted.append({\n",
    "            \"score\": 0.50,\n",
    "            \"text\": r.get(\"text\", \"\"),\n",
    "            \"source\": r.get(\"source\", \"\"),\n",
    "            \"chunk_id\": r.get(\"chunk_id\", \"\"),\n",
    "            \"metadata\": dict(r),\n",
    "            \"source_type\": \"graph\"\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def hybrid_retrieval_pipeline(query: str, top_k_semantic: int = DEFAULT_TOP_K, top_k_final: int = FINAL_TOP_K) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Combined hybrid retrieval:\n",
    "      1) Vector retrieval (BGE-M3 / Qdrant)\n",
    "      2) Graph retrieval (Memgraph)\n",
    "      3) Merge, deduplicate by chunk_id, and return top-K by score\n",
    "    \"\"\"\n",
    "    sem = semantic_retrieval(query, top_k=top_k_semantic)\n",
    "    gr = graph_retrieval(query, memgraph_conn=memgraph, limit=top_k_semantic)\n",
    "\n",
    "    # Merge preserving best score seen\n",
    "    merged: Dict[str, Dict[str, Any]] = {}\n",
    "    for item in sem + gr:\n",
    "        cid = item.get(\"chunk_id\") or item.get(\"metadata\", {}).get(\"chunk_id\") or item.get(\"source\") + \"_\" + str(len(merged))\n",
    "        if cid in merged:\n",
    "            # keep higher score\n",
    "            if item.get(\"score\", 0.0) > merged[cid].get(\"score\", 0.0):\n",
    "                merged[cid] = item\n",
    "        else:\n",
    "            merged[cid] = item\n",
    "\n",
    "    merged_list = list(merged.values())\n",
    "    merged_list = sorted(merged_list, key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "\n",
    "    # Trim to final top_k_final\n",
    "    return merged_list[:top_k_final]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Quick test (replace sample_query as needed)\n",
    "# -------------------------\n",
    "sample_query = \"What does 6G offer?\"\n",
    "hybrid_results = hybrid_retrieval_pipeline(sample_query, top_k_semantic=20, top_k_final=10)\n",
    "\n",
    "print(f\"Retrieved {len(hybrid_results)} hybrid candidates for: {sample_query}\\n\")\n",
    "for i, doc in enumerate(hybrid_results, 1):\n",
    "    print(f\"Result {i}: [{doc.get('source_type')}] score={doc.get('score'):.4f} source={doc.get('source')}\")\n",
    "    print(doc.get('text', '')[:300].replace(\"\\n\", \" \"), \"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd92a9e",
   "metadata": {},
   "source": [
    "# 4 Semantic Reranking\n",
    "\n",
    "(after hybrid retrieval)\n",
    "\n",
    "This cell takes everything retrieved in Cell 3, and reorders the results using a cross-encoder, which computes true, pairwise relevance between the query and each candidate chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df735890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranker batches: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 reranked results:\n",
      "\n",
      "1. chunk_id=test_chunk8 source_type=vector\n",
      "   final_score=3.2346 rerank_score=3.6906 orig_score=0.6511\n",
      "   snippet: . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc ...\n",
      "\n",
      "2. chunk_id=test_chunk7 source_type=vector\n",
      "   final_score=2.5895 rerank_score=2.9295 orig_score=0.6629\n",
      "   snippet: . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit ...\n",
      "\n",
      "3. chunk_id=test_chunk19 source_type=vector\n",
      "   final_score=1.6258 rerank_score=1.8077 orig_score=0.5954\n",
      "   snippet: . To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy lean systems and network sharing; 2) responsible ICT, that ensures transparency and traceability, secures human rights, and guarantees children online protection; 3) ICT for inclu ...\n",
      "\n",
      "4. chunk_id=test_chunk6 source_type=vector\n",
      "   final_score=0.1984 rerank_score=0.1183 orig_score=0.6524\n",
      "   snippet: . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis ...\n",
      "\n",
      "5. chunk_id=test_chunk67 source_type=vector\n",
      "   final_score=-0.2659 rerank_score=-0.4255 orig_score=0.6384\n",
      "   snippet: . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Robust Cross-Encoder Reranking cell\n",
    "# Inputs:\n",
    "#   - query (str)\n",
    "#   - hybrid_results (list of dicts, each must have at least 'text' and ideally 'score' and 'chunk_id')\n",
    "# Outputs:\n",
    "#   - reranked_results (list of dicts) with fields: rerank_score, final_score, original metadata\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Config ----------\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANKER_BATCH_SIZE = 16     # lower this (e.g., 4) on small-CPU machines\n",
    "MAX_CHARS_FOR_RERANKER = 2000  # truncate long passages to this many chars\n",
    "ALPHA = 0.85                 # final_score = ALPHA * rerank_score + (1-ALPHA) * original_score\n",
    "# ----------------------------\n",
    "\n",
    "# Load reranker if not loaded already\n",
    "try:\n",
    "    # If you already have 'reranker' in the namespace, reuse it\n",
    "    reranker\n",
    "except NameError:\n",
    "    try:\n",
    "        print(f\"Loading reranker model: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL)\n",
    "    except Exception as e:\n",
    "        reranker = None\n",
    "        print(\"Could not load reranker model:\", e)\n",
    "\n",
    "def _safe_text(txt: str, max_chars: int = MAX_CHARS_FOR_RERANKER) -> str:\n",
    "    \"\"\"Make sure text is a string and truncate it preserving head+tail if too long.\"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    txt = txt.strip()\n",
    "    if len(txt) <= max_chars:\n",
    "        return txt\n",
    "    half = max_chars // 2\n",
    "    return txt[:half] + \"\\n\\n[...] \\n\\n\" + txt[-half:]\n",
    "\n",
    "def rerank_results(query: str, results: List[Dict[str, Any]], top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rerank `results` (list of dicts with 'text' and 'score') using a CrossEncoder.\n",
    "    Returns top_n reranked items, with 'rerank_score' and 'final_score' added.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    # If no reranker available, fallback to original sorting\n",
    "    if reranker is None:\n",
    "        print(\"Reranker unavailable — returning results sorted by original score.\")\n",
    "        return sorted(results, key=lambda x: x.get(\"score\", 0.0), reverse=True)[:top_n]\n",
    "\n",
    "    # Prepare safe (query, text) pairs\n",
    "    pairs = []\n",
    "    index_map = []  # map pair index -> result index\n",
    "    for i, r in enumerate(results):\n",
    "        text = r.get(\"text\", \"\")\n",
    "        safe_text = _safe_text(text)\n",
    "        pairs.append((query, safe_text))\n",
    "        index_map.append(i)\n",
    "\n",
    "    # Batch-predict scores to avoid OOM\n",
    "    rerank_scores = []\n",
    "    for i in tqdm(range(0, len(pairs), RERANKER_BATCH_SIZE), desc=\"Reranker batches\"):\n",
    "        batch = pairs[i : i + RERANKER_BATCH_SIZE]\n",
    "        try:\n",
    "            batch_scores = reranker.predict(batch, show_progress_bar=False)\n",
    "        except TypeError:\n",
    "            # Some cross-encoder versions expect list[str] instead of list[tuple]\n",
    "            # convert to \"query \\t text\" fallback\n",
    "            batch_joined = [q + \"\\t\" + t for q, t in batch]\n",
    "            batch_scores = reranker.predict(batch_joined, show_progress_bar=False)\n",
    "        # ensure list of floats\n",
    "        batch_scores = np.asarray(batch_scores, dtype=float).tolist()\n",
    "        rerank_scores.extend(batch_scores)\n",
    "\n",
    "    # Attach reranker scores back to original results and compute final score\n",
    "    reranked = []\n",
    "    for pair_idx, score in enumerate(rerank_scores):\n",
    "        res_idx = index_map[pair_idx]\n",
    "        orig = results[res_idx].copy()\n",
    "        orig_score = float(orig.get(\"score\", 0.0))\n",
    "        orig[\"rerank_score\"] = float(score)\n",
    "        orig[\"final_score\"] = float(ALPHA * orig[\"rerank_score\"] + (1.0 - ALPHA) * orig_score)\n",
    "        reranked.append(orig)\n",
    "\n",
    "    # Sort by final_score descending and return top_n\n",
    "    reranked_sorted = sorted(reranked, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    return reranked_sorted[:top_n]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage (run after hybrid_retrieval_pipeline)\n",
    "# ---------------------------\n",
    "query = \"What does 6G offer?\"\n",
    "try:\n",
    "    reranked_results = rerank_results(query, hybrid_results, top_n=5)\n",
    "    print(f\"Top {len(reranked_results)} reranked results:\")\n",
    "    for i, r in enumerate(reranked_results, 1):\n",
    "        print(f\"\\n{i}. chunk_id={r.get('chunk_id', 'n/a')} source_type={r.get('source_type','?')}\")\n",
    "        print(f\"   final_score={r['final_score']:.4f} rerank_score={r['rerank_score']:.4f} orig_score={r.get('score', 0.0):.4f}\")\n",
    "        print(\"   snippet:\", r.get(\"text\",\"\")[:300].replace(\"\\n\",\" \"), \"...\")\n",
    "except Exception as e:\n",
    "    print(\"Reranking failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033fac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Mini evaluation dataset\n",
    "# ------------------------\n",
    "# Format: {\"query\": \"...\", \"gold_keywords\": [\"word1\", \"word2\", ...]}\n",
    "\n",
    "evaluation_set = [\n",
    "    {\n",
    "        \"query\": \"What does 6G offer?\",\n",
    "        \"gold_keywords\": [\"sub-thz\", \"terahertz\", \"ai-native\", \"ai native\", \n",
    " \"extreme bandwidth\", \"ultra-reliable\", \"low-latency\",\n",
    " \"massive connectivity\", \"holographic\", \"ris\", \"b5g\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is RIS technology?\",\n",
    "        \"gold_keywords\": [\"reconfigurable intelligent surface\", \"ris\", \n",
    " \"metasurface\", \"reflective element\", \"beamforming\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What challenges does 5G face?\",\n",
    "        \"gold_keywords\": [\"latency\", \"energy efficiency\", \"spectrum\", \n",
    " \"massive iot\", \"ultra reliable\", \"coverage\"]\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(evaluation_set)} evaluation samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee421f98",
   "metadata": {},
   "source": [
    "# 5. Final Context + Citation Packaging\n",
    "In this last cell we take the relevance-boosted results (semantic reranking) and out put a context dic for LLM to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4d9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Context Block Ready\n",
      "\n",
      ". In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resource-efficient connected agriculture, universal access to digital healthcare, etc. This requires embedded autonomous sensors and actuators, worldwide coverage with outstanding energy-, material-, and cost-efficiency, as well as a network platform with high availability and security [3]. III\n",
      "\n",
      "---\n",
      "\n",
      ". The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physi\n",
      "\n",
      "Citations:\n",
      "- ./data/test.pdf | . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requ...\n",
      "- ./data/test.pdf | . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged....\n",
      "- ./data/test.pdf | . To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy...\n",
      "- ./data/test.pdf | . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This i...\n",
      "- ./data/test.pdf | . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision....\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Cell 5 (Fixed): Final Context + Citations Packaging (BGE-M3)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_and_deduplicate_structured(items: list):\n",
    "    \"\"\"\n",
    "    Deduplicate based on cleaned text, but keep alignment\n",
    "    between text, source, rerank score, etc.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "\n",
    "    for item in items:\n",
    "        text = item[\"text\"]\n",
    "        key = re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(item)\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def assemble_context(reranked_results: list, max_chars: int = 8000):\n",
    "    \"\"\"\n",
    "    Merge reranked chunks into final LLM context.\n",
    "    Now correctly handles deduplication while preserving metadata alignment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Deduplicate the full items, not just the text\n",
    "    unique_items = clean_and_deduplicate_structured(reranked_results)\n",
    "\n",
    "    merged_text = \"\"\n",
    "    citations = []\n",
    "\n",
    "    for item in unique_items:\n",
    "        text = item[\"text\"].strip()\n",
    "\n",
    "        # Stop once we exceed context budget\n",
    "        if len(merged_text) + len(text) > max_chars:\n",
    "            break\n",
    "\n",
    "        merged_text += text + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        citations.append({\n",
    "            \"source\": item[\"source\"],\n",
    "            \"chunk_preview\": text[:120] + \"...\",\n",
    "            \"rerank_score\": item.get(\"rerank_score\"),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"merged_context\": merged_text.strip(),\n",
    "        \"citations\": citations,\n",
    "        \"raw_chunks\": [item[\"text\"] for item in unique_items]\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Test final context assembly\n",
    "# ---------------------------\n",
    "context_for_llm = assemble_context(reranked_results, max_chars=8000)\n",
    "\n",
    "print(\"Final Context Block Ready\\n\")\n",
    "print(context_for_llm[\"merged_context\"][:800])  # preview first 800 chars\n",
    "print(\"\\nCitations:\")\n",
    "for c in context_for_llm[\"citations\"]:\n",
    "    print(f\"- {c['source']} | {c['chunk_preview']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e23f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved context_for_llm to: /home/diego/Projects/advanced-multisource-rag-project/data/context_for_llm.json\n"
     ]
    }
   ],
   "source": [
    "from config import CONTEXT_FILE\n",
    "import json\n",
    "\n",
    "with open(CONTEXT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(context_for_llm, f, indent=2)\n",
    "    \n",
    "print(\"Saved context_for_llm to:\", CONTEXT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30986c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
