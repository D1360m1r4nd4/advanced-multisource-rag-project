{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346c42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Using fallback HTTP Ollama client (requests).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports + Global Settings\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Optional: pretty printing\n",
    "import textwrap\n",
    "\n",
    "# Load Ollama local LLM client (try the official package first, fall back to HTTP)\n",
    "# Preferred: `pip install ollama`\n",
    "llm = None\n",
    "try:\n",
    "    from ollama import Client as OllamaClient\n",
    "    llm = OllamaClient(host=\"http://localhost:11434\")\n",
    "    print(\"üî• Local LLM client ready (ollama package).\")\n",
    "except Exception as e:\n",
    "    # Fallback: lightweight HTTP client using requests\n",
    "    try:\n",
    "        import requests\n",
    "        class SimpleOllamaClient:\n",
    "            def __init__(self, host='http://localhost:11434'):\n",
    "                self.host = host.rstrip('/')\n",
    "            def chat(self, model, messages):\n",
    "                # maps to Ollama's chat endpoint\n",
    "                url = f\"{self.host}/api/chat\"\n",
    "                payload = {\"model\": model, \"messages\": messages}\n",
    "                resp = requests.post(url, json=payload, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                return resp.json()\n",
    "        llm = SimpleOllamaClient(host=\"http://localhost:11434\")\n",
    "        print(\"‚ö†Ô∏è Using fallback HTTP Ollama client (requests).\")\n",
    "    except Exception as e2:\n",
    "        print(\"‚ö†Ô∏è Could not import `ollama` package or use HTTP fallback.\")\n",
    "        print(\"To enable the local LLM client, either:\")\n",
    "        print(\"  1) pip install ollama\")\n",
    "        print(\"  2) ensure Ollama daemon is running at http://localhost:11434 and install requests (`pip install requests`)\")\n",
    "        print(\"Falling back to a stub llm object that raises if used.\")\n",
    "        class _StubLLM:\n",
    "            def chat(self, *args, **kwargs):\n",
    "                raise RuntimeError(\"No Ollama client available. Install `ollama` or `requests` and start the Ollama daemon.\")\n",
    "        llm = _StubLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ecc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# üìò Notebook 04 ‚Äî Cell 2\n",
    "# Prompt Template + Formatting Function\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def build_rag_prompt(query: str, context_data: Dict):\n",
    "    \"\"\"\n",
    "    Build the final RAG prompt to send to the local LLM.\n",
    "    Includes:\n",
    "    - merged context\n",
    "    - user question\n",
    "    - instruction to use citations\n",
    "    \"\"\"\n",
    "\n",
    "    context_block = context_data[\"merged_context\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant answering questions using ONLY the context provided.\n",
    "\n",
    "CONTEXT:\n",
    "-------\n",
    "{context_block}\n",
    "-------\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use ONLY the information in the context.\n",
    "- Do NOT hallucinate.\n",
    "- Cite your sources using this format: [source].\n",
    "- If the answer is not in the context, say: \"The answer is not available in the provided context.\"\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "FINAL ANSWER (with citations):\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74611dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Variable 'context_for_llm' not found in kernel.\n",
      "Please run Notebook 03 first to generate the retrieval context, then return here.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# üìò Notebook 04 ‚Äî Cell 3\n",
    "# Generate Answer using Local LLM (Ollama)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def generate_llm_answer(query: str, context_for_llm: Dict, model: str = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Sends the final prompt to the local LLM and retrieves the response.\n",
    "    \"\"\"\n",
    "    prompt = build_rag_prompt(query, context_for_llm)\n",
    "\n",
    "    response = llm.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    return answer\n",
    "\n",
    "\n",
    "# ---- Test the answering pipeline (only if variables are defined) ----\n",
    "# If 'context_for_llm' comes from notebook 03, use it. Otherwise, use a sample query.\n",
    "if 'context_for_llm' in locals():\n",
    "    # Use the query and context from notebook 03\n",
    "    sample_answer = generate_llm_answer(query, context_for_llm, model=\"llama3\")\n",
    "    \n",
    "    print(\"\\n===== LLM ANSWER =====\\n\")\n",
    "    print(sample_answer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Variable 'context_for_llm' not found in kernel.\")\n",
    "    print(\"Please run Notebook 03 first to generate the retrieval context, then return here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f556dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Variables 'sample_answer' and/or 'context_for_llm' not found.\n",
      "Please run cells above first to generate these variables.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# üìò Notebook 04 ‚Äî Cell 4\n",
    "# Final Output Formatting (Answer + Citations)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def format_final_output(answer: str, context_for_llm: Dict):\n",
    "    \"\"\"\n",
    "    Returns a clean, structured output including:\n",
    "    - Answer\n",
    "    - Citations\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"üìò FINAL ANSWER\")\n",
    "    print(\"=======================\\n\")\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"üîó CITATIONS\")\n",
    "    print(\"=======================\\n\")\n",
    "    for c in context_for_llm[\"citations\"]:\n",
    "        print(f\"- Source: {c['source']}\")\n",
    "        print(f\"  Preview: {c['chunk_preview']}\")\n",
    "        print(f\"  Score: {c['rerank_score']:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Display final result (only if variables are defined)\n",
    "if 'sample_answer' in locals() and 'context_for_llm' in locals():\n",
    "    format_final_output(sample_answer, context_for_llm)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Variables 'sample_answer' and/or 'context_for_llm' not found.\")\n",
    "    print(\"Please run cells above first to generate these variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11206f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
