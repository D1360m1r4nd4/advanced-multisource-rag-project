{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b9a18",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline Orchestration + Reranking\n",
    "\n",
    "**Goal**\n",
    "Build a hybrid retrieval system that:\n",
    "\n",
    "1. Pulls candidate results from your Qdrant vector store (semantic retrieval).\n",
    "\n",
    "2. Optionally augments those with graph-based or metadata filters (hierarchical or relational layer).\n",
    "\n",
    "3. Applies cross-encoder reranking to refine relevance before passing context to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d494c",
   "metadata": {},
   "source": [
    "# 1: Setup & Connections\n",
    "\n",
    "* Import required libraries.\n",
    "* Load/connect to Qdrant (vector DB) and Memgraph (graph DB).\n",
    "* Load the embedding model (for encoding queries) and the cross-encoder reranker (for re-scoring * candidate documents).\n",
    "* Define some configuration variables (collection name, top_k defaults).\n",
    "* Print short status messages so you know everything connected/loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb589d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Qdrant at http://localhost:6333. Collections: ['enterprise_docs']\n",
      "Connected to Memgraph at localhost:7687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd12687f13740659b8b8a998a35cc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BGE-M3 embedder: BAAI/bge-m3\n",
      "Loaded reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "--- Summary ---\n",
      "Qdrant client: OK\n",
      "Memgraph client: OK\n",
      "Embedder: OK (BGE-M3)\n",
      "Reranker: OK (cross-encoder/ms-marco-MiniLM-L-6-v2)\n",
      "Collection in use: enterprise_docs\n",
      "Retrieval settings: DEFAULT_TOP_K=50, FINAL_TOP_K=5\n"
     ]
    }
   ],
   "source": [
    "# Before running this notebook, ensure Notebooks 01–02 are done\n",
    "# Qdrant + Memgraph must be running\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "# Memgraph\n",
    "from gqlalchemy import Memgraph\n",
    "\n",
    "# NEW: BGE-M3 embedder\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# Cross-encoder for reranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# ----- Configuration -----\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "MEMGRAPH_HOST = os.getenv(\"MEMGRAPH_HOST\", \"localhost\")\n",
    "MEMGRAPH_PORT = int(os.getenv(\"MEMGRAPH_PORT\", 7687))\n",
    "\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION\", \"enterprise_docs\")\n",
    "\n",
    "DEFAULT_TOP_K = 50   # initial candidates\n",
    "FINAL_TOP_K = 5      # final reranked results\n",
    "\n",
    "\n",
    "# ----- Connect to Qdrant -----\n",
    "\n",
    "try:\n",
    "    qdrant_client = QdrantClient(url=QDRANT_URL)\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"Connected to Qdrant at {QDRANT_URL}. Collections: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    qdrant_client = None\n",
    "    print(\"Could not connect to Qdrant:\", e)\n",
    "\n",
    "\n",
    "# ----- Connect to Memgraph -----\n",
    "\n",
    "try:\n",
    "    memgraph = Memgraph(host=MEMGRAPH_HOST, port=MEMGRAPH_PORT)\n",
    "    test = list(memgraph.execute_and_fetch(\"RETURN 1 AS ok\"))\n",
    "    print(f\"Connected to Memgraph at {MEMGRAPH_HOST}:{MEMGRAPH_PORT}\")\n",
    "except Exception as e:\n",
    "    memgraph = None\n",
    "    print(\"Could not connect to Memgraph:\", e)\n",
    "\n",
    "\n",
    "# ----- Load BGE-M3 Embedder -----\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "\n",
    "try:\n",
    "    embedder = BGEM3FlagModel(EMBEDDING_MODEL_NAME, use_fp16=False)\n",
    "    print(f\"Loaded BGE-M3 embedder: {EMBEDDING_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    embedder = None\n",
    "    print(\"Could not load BGE-M3 embedder:\", e)\n",
    "\n",
    "\n",
    "# ----- Query embedding helper (BGE-M3) -----\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    \"\"\"Return normalized 1024-d query embedding using BGE-M3.\"\"\"\n",
    "    out = embedder.encode(\n",
    "        text,\n",
    "        max_length=8192,\n",
    "        return_dense=True,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    vec = out[\"dense_vecs\"]\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0:\n",
    "        vec = vec / norm\n",
    "    return vec.tolist()\n",
    "\n",
    "\n",
    "# ----- Load reranker -----\n",
    "\n",
    "RERANKER_MODEL_NAME = os.getenv(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "try:\n",
    "    reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
    "    print(f\"Loaded reranker: {RERANKER_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    reranker = None\n",
    "    print(\"Could not load reranker:\", e)\n",
    "\n",
    "\n",
    "# ----- Summary -----\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Qdrant client: {'OK' if qdrant_client else 'MISSING'}\")\n",
    "print(f\"Memgraph client: {'OK' if memgraph else 'MISSING'}\")\n",
    "print(f\"Embedder: {'OK' if embedder else 'MISSING'} (BGE-M3)\")\n",
    "print(f\"Reranker: {'OK' if reranker else 'MISSING'} ({RERANKER_MODEL_NAME})\")\n",
    "print(f\"Collection in use: {COLLECTION_NAME}\")\n",
    "print(f\"Retrieval settings: DEFAULT_TOP_K={DEFAULT_TOP_K}, FINAL_TOP_K={FINAL_TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd2eb7",
   "metadata": {},
   "source": [
    "# 2 Semantic Retrieval from Qdrant\n",
    "\n",
    "Here we will:\n",
    "1. Take a natural language query.\n",
    "2. Encode it into a vector using your embedding model.\n",
    "3. Search for the most semantically similar chunks (documents) in Qdrant.\n",
    "4. Return and display the top K results, with metadata for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d02be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results for query: 'What does 6G offer?'\n",
      "\n",
      "Result 1: (score=0.6629)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit...\n",
      "\n",
      "Result 2: (score=0.6524)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis...\n",
      "\n",
      "Result 3: (score=0.6511)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc...\n",
      "\n",
      "Result 4: (score=0.6384)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n...\n",
      "\n",
      "Result 5: (score=0.6373)\n",
      "Source: ./data/test.pdf | Page: None\n",
      "Snippet: . M. Tatipamula is with Ericsson Silicon Valley, Santa Clara, CA 95054, USA (e-mail: mallik.tatipamula@ericsson.com). 3202 nu J 6 ]IN.sc[ 2 v 23800.6032:vi Xra 1 Towards 6 G: evolution in the Making atipamula, and Muhammad Ali Imran systems toward their limits within 10 years of their launch and cal...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_retrieval(query: str, top_k: int = 5, collection_name: str = COLLECTION_NAME):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant documents from Qdrant using BGE-M3 embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Embed query using BGE-M3 ---\n",
    "    query_vector = embed_query(query)   # returns normalized 1024-d list\n",
    "\n",
    "    # --- 2) Qdrant vector search using new API ---\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "\n",
    "    # --- 3) Format results ---\n",
    "    formatted = []\n",
    "    for point in results.points:\n",
    "        formatted.append({\n",
    "            \"score\": point.score,\n",
    "            \"text\": point.payload.get(\"text\", \"\"),\n",
    "            \"source\": point.payload.get(\"source\", \"\"),\n",
    "            \"chunk_id\": point.payload.get(\"chunk_id\", \"\"),\n",
    "            \"metadata\": point.payload\n",
    "        })\n",
    "\n",
    "    return formatted\n",
    "\n",
    "# ===========================================\n",
    "# Example Query — try one from your dataset\n",
    "# Replace this with a question relevant to your data.\n",
    "sample_query = \"What does 6G offer?\"\n",
    "\n",
    "# Run the retrieval function\n",
    "retrieved_docs = semantic_retrieval(sample_query, top_k=5)\n",
    "\n",
    "# Display retrieved documents\n",
    "print(f\"\\nTop {len(retrieved_docs)} results for query: '{sample_query}'\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i}: (score={doc['score']:.4f})\")\n",
    "    print(f\"Source: {doc['source']} | Page: {doc.get('page')}\")\n",
    "    print(f\"Snippet: {doc['text'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038b188",
   "metadata": {},
   "source": [
    "# 3 Hybrid Retrieval (Vector + Graph)\n",
    "This next cell implements a hybrid retrieval layer that merges: \n",
    "1. **Semantic relevance**\n",
    "2. **Graph-based context**\n",
    "\n",
    "This hybrid approach improves entreprise knowledge retrieval by connecting related entities instead of relying exclusively on text similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c12fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 hybrid candidates for: What does 6G offer?\n",
      "\n",
      "Result 1: [vector] score=0.6629 source=./data/test.pdf\n",
      ". The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit \n",
      "--------------------------------------------------------------------------------\n",
      "Result 2: [vector] score=0.6524 source=./data/test.pdf\n",
      ". II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis \n",
      "--------------------------------------------------------------------------------\n",
      "Result 3: [vector] score=0.6511 source=./data/test.pdf\n",
      ". In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc \n",
      "--------------------------------------------------------------------------------\n",
      "Result 4: [vector] score=0.6384 source=./data/test.pdf\n",
      ". This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n \n",
      "--------------------------------------------------------------------------------\n",
      "Result 5: [vector] score=0.6373 source=./data/test.pdf\n",
      ". M. Tatipamula is with Ericsson Silicon Valley, Santa Clara, CA 95054, USA (e-mail: mallik.tatipamula@ericsson.com). 3202 nu J 6 ]IN.sc[ 2 v 23800.6032:vi Xra 1 Towards 6 G: evolution in the Making atipamula, and Muhammad Ali Imran systems toward their limits within 10 years of their launch and cal \n",
      "--------------------------------------------------------------------------------\n",
      "Result 6: [vector] score=0.6184 source=./data/test.pdf\n",
      ". III. DRIVING TRENDS AND USE CASES (UCS) The prospected 6 G promise stems from four main driving UCs, which both lay the foundation for numerous applications and set the technical requirements for future communication systems. These UCs are shown in Fig. 1 and are detailed in this section. Internet \n",
      "--------------------------------------------------------------------------------\n",
      "Result 7: [vector] score=0.6058 source=./data/test.pdf\n",
      ". and diversified services settings, transparency and affordance, collaboration and communication, access and privacy, and a range of interaction types ranging from real-time interactive to highly asynchronous [2]. Finally, Fig. 5 shows several stepping stones, linking the Io Mus T journey with the  \n",
      "--------------------------------------------------------------------------------\n",
      "Result 8: [vector] score=0.6028 source=./data/test.pdf\n",
      ". As such, a fundamental enabling technology is network adaptability, which refers to the idea of enabling rapid network deployments and the fast introduction of new services. This comprises dynamic network deployments, which includes ad-hoc or temporal deployments and mobile and non-terrestrial net \n",
      "--------------------------------------------------------------------------------\n",
      "Result 9: [vector] score=0.5954 source=./data/test.pdf\n",
      ". To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy lean systems and network sharing; 2) responsible ICT, that ensures transparency and traceability, secures human rights, and guarantees children online protection; 3) ICT for inclu \n",
      "--------------------------------------------------------------------------------\n",
      "Result 10: [vector] score=0.5920 source=./data/test.pdf\n",
      "The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstract—While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (Io T), the unprecedented strain on communication networks put by t \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fixed semantic_retrieval + hybrid_retrieval_pipeline\n",
    "# Uses: BGE-M3 embedder and new Qdrant client API (query_points)\n",
    "# -------------------------\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def semantic_retrieval(query: str, top_k: int = 20, collection_name: str = COLLECTION_NAME) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k vector matches from Qdrant using BGE-M3 query embedding.\n",
    "    Uses qdrant_client.query_points(...) (new client API).\n",
    "    Returns a list of dicts with keys: score, text, source, chunk_id, source_type.\n",
    "    \"\"\"\n",
    "    if qdrant_client is None:\n",
    "        print(\"⚠️ Qdrant client not connected — skipping semantic retrieval.\")\n",
    "        return []\n",
    "\n",
    "    if embedder is None:\n",
    "        print(\"⚠️ Embedder not loaded — cannot run semantic retrieval.\")\n",
    "        return []\n",
    "\n",
    "    # 1) Embed the query consistently with how chunks were embedded\n",
    "    out = embedder.encode(\n",
    "        query,\n",
    "        max_length=8192,\n",
    "        return_dense=True,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "\n",
    "    qvec = out[\"dense_vecs\"]\n",
    "    # ensure numpy array -> normalize -> list\n",
    "    qvec = np.asarray(qvec, dtype=float)\n",
    "    norm = np.linalg.norm(qvec)\n",
    "    if norm > 0:\n",
    "        qvec = (qvec / norm).tolist()\n",
    "    else:\n",
    "        qvec = qvec.tolist()\n",
    "\n",
    "    # 2) Use the new Qdrant API to query points\n",
    "    # Note: some qdrant-client versions expose query_points(...) which returns an object with .points\n",
    "    try:\n",
    "        resp = qdrant_client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=qvec,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "    except AttributeError:\n",
    "        # Fallback for slightly different client API names\n",
    "        resp = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=qvec,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "    # 3) Normalize/form the response\n",
    "    formatted: List[Dict[str, Any]] = []\n",
    "    # If resp has .points (new API)\n",
    "    points = getattr(resp, \"points\", None)\n",
    "    if points is None:\n",
    "        # older return type: resp might already be a list\n",
    "        points = resp\n",
    "\n",
    "    for p in points:\n",
    "        # p may be a Point object or a dict depending on client version\n",
    "        score = getattr(p, \"score\", None)\n",
    "        payload = getattr(p, \"payload\", None) or (p.get(\"payload\") if isinstance(p, dict) else None)\n",
    "\n",
    "        formatted.append({\n",
    "            \"score\": float(score) if score is not None else 0.0,\n",
    "            \"text\": payload.get(\"text\", \"\") if payload else \"\",\n",
    "            \"source\": payload.get(\"source\", \"\") if payload else \"\",\n",
    "            \"chunk_id\": payload.get(\"chunk_id\", \"\") if payload else \"\",\n",
    "            \"metadata\": payload or {},\n",
    "            \"source_type\": \"vector\"\n",
    "        })\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def graph_retrieval(query: str, memgraph_conn: Memgraph, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Safe) graph retrieval to fetch chunks from Memgraph matching the query.\n",
    "    Keeps same output schema as semantic_retrieval.\n",
    "    \"\"\"\n",
    "    if memgraph_conn is None:\n",
    "        return []\n",
    "\n",
    "    cypher_query = f\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE toLower(c.text) CONTAINS toLower(\"{query}\")\n",
    "    RETURN c.text AS text, c.source AS source, c.chunk_id AS chunk_id\n",
    "    LIMIT {limit};\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        rows = list(memgraph_conn.execute_and_fetch(cypher_query))\n",
    "    except Exception as e:\n",
    "        print(\"Graph retrieval failed:\", e)\n",
    "        return []\n",
    "\n",
    "    formatted = []\n",
    "    for r in rows:\n",
    "        formatted.append({\n",
    "            \"score\": 0.50,\n",
    "            \"text\": r.get(\"text\", \"\"),\n",
    "            \"source\": r.get(\"source\", \"\"),\n",
    "            \"chunk_id\": r.get(\"chunk_id\", \"\"),\n",
    "            \"metadata\": dict(r),\n",
    "            \"source_type\": \"graph\"\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def hybrid_retrieval_pipeline(query: str, top_k_semantic: int = DEFAULT_TOP_K, top_k_final: int = FINAL_TOP_K) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Combined hybrid retrieval:\n",
    "      1) Vector retrieval (BGE-M3 / Qdrant)\n",
    "      2) Graph retrieval (Memgraph)\n",
    "      3) Merge, deduplicate by chunk_id, and return top-K by score\n",
    "    \"\"\"\n",
    "    sem = semantic_retrieval(query, top_k=top_k_semantic)\n",
    "    gr = graph_retrieval(query, memgraph_conn=memgraph, limit=top_k_semantic)\n",
    "\n",
    "    # Merge preserving best score seen\n",
    "    merged: Dict[str, Dict[str, Any]] = {}\n",
    "    for item in sem + gr:\n",
    "        cid = item.get(\"chunk_id\") or item.get(\"metadata\", {}).get(\"chunk_id\") or item.get(\"source\") + \"_\" + str(len(merged))\n",
    "        if cid in merged:\n",
    "            # keep higher score\n",
    "            if item.get(\"score\", 0.0) > merged[cid].get(\"score\", 0.0):\n",
    "                merged[cid] = item\n",
    "        else:\n",
    "            merged[cid] = item\n",
    "\n",
    "    merged_list = list(merged.values())\n",
    "    merged_list = sorted(merged_list, key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "\n",
    "    # Trim to final top_k_final\n",
    "    return merged_list[:top_k_final]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Quick test (replace sample_query as needed)\n",
    "# -------------------------\n",
    "sample_query = \"What does 6G offer?\"\n",
    "hybrid_results = hybrid_retrieval_pipeline(sample_query, top_k_semantic=20, top_k_final=10)\n",
    "\n",
    "print(f\"Retrieved {len(hybrid_results)} hybrid candidates for: {sample_query}\\n\")\n",
    "for i, doc in enumerate(hybrid_results, 1):\n",
    "    print(f\"Result {i}: [{doc.get('source_type')}] score={doc.get('score'):.4f} source={doc.get('source')}\")\n",
    "    print(doc.get('text', '')[:300].replace(\"\\n\", \" \"), \"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd92a9e",
   "metadata": {},
   "source": [
    "# 4 Semantic Reranking\n",
    "\n",
    "(after hybrid retrieval)\n",
    "\n",
    "This cell takes everything retrieved in Cell 3, and reorders the results using a cross-encoder, which computes true, pairwise relevance between the query and each candidate chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df735890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranker batches: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 reranked results:\n",
      "\n",
      "1. chunk_id=test_chunk8 source_type=vector\n",
      "   final_score=3.2346 rerank_score=3.6906 orig_score=0.6511\n",
      "   snippet: . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resourc ...\n",
      "\n",
      "2. chunk_id=test_chunk7 source_type=vector\n",
      "   final_score=2.5895 rerank_score=2.9295 orig_score=0.6629\n",
      "   snippet: . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physical world of senses, actions, and experiences, and its programmable digital representation. Wit ...\n",
      "\n",
      "3. chunk_id=test_chunk19 source_type=vector\n",
      "   final_score=1.6258 rerank_score=1.8077 orig_score=0.5954\n",
      "   snippet: . To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy lean systems and network sharing; 2) responsible ICT, that ensures transparency and traceability, secures human rights, and guarantees children online protection; 3) ICT for inclu ...\n",
      "\n",
      "4. chunk_id=test_chunk6 source_type=vector\n",
      "   final_score=0.1984 rerank_score=0.1183 orig_score=0.6524\n",
      "   snippet: . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This is followed by introducing the paradigm shifts that formulate an evolved network architecture in Sec. IV. In Sec. V, we highlight the main 6 G technologies needed to realize the vis ...\n",
      "\n",
      "5. chunk_id=test_chunk67 source_type=vector\n",
      "   final_score=-0.2659 rerank_score=-0.4255 orig_score=0.6384\n",
      "   snippet: . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision. To this end, we presented the 6 G promise, portrayed through four driving UCs. To achieve the discussed promise, we introduced our vision of the six architectural pillars of 6 G n ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Robust Cross-Encoder Reranking cell\n",
    "# Inputs:\n",
    "#   - query (str)\n",
    "#   - hybrid_results (list of dicts, each must have at least 'text' and ideally 'score' and 'chunk_id')\n",
    "# Outputs:\n",
    "#   - reranked_results (list of dicts) with fields: rerank_score, final_score, original metadata\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Config ----------\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANKER_BATCH_SIZE = 16     # lower this (e.g., 4) on small-CPU machines\n",
    "MAX_CHARS_FOR_RERANKER = 2000  # truncate long passages to this many chars\n",
    "ALPHA = 0.85                 # final_score = ALPHA * rerank_score + (1-ALPHA) * original_score\n",
    "# ----------------------------\n",
    "\n",
    "# Load reranker if not loaded already\n",
    "try:\n",
    "    # If you already have 'reranker' in the namespace, reuse it\n",
    "    reranker\n",
    "except NameError:\n",
    "    try:\n",
    "        print(f\"Loading reranker model: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL)\n",
    "    except Exception as e:\n",
    "        reranker = None\n",
    "        print(\"Could not load reranker model:\", e)\n",
    "\n",
    "def _safe_text(txt: str, max_chars: int = MAX_CHARS_FOR_RERANKER) -> str:\n",
    "    \"\"\"Make sure text is a string and truncate it preserving head+tail if too long.\"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    txt = txt.strip()\n",
    "    if len(txt) <= max_chars:\n",
    "        return txt\n",
    "    half = max_chars // 2\n",
    "    return txt[:half] + \"\\n\\n[...] \\n\\n\" + txt[-half:]\n",
    "\n",
    "def rerank_results(query: str, results: List[Dict[str, Any]], top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rerank `results` (list of dicts with 'text' and 'score') using a CrossEncoder.\n",
    "    Returns top_n reranked items, with 'rerank_score' and 'final_score' added.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    # If no reranker available, fallback to original sorting\n",
    "    if reranker is None:\n",
    "        print(\"Reranker unavailable — returning results sorted by original score.\")\n",
    "        return sorted(results, key=lambda x: x.get(\"score\", 0.0), reverse=True)[:top_n]\n",
    "\n",
    "    # Prepare safe (query, text) pairs\n",
    "    pairs = []\n",
    "    index_map = []  # map pair index -> result index\n",
    "    for i, r in enumerate(results):\n",
    "        text = r.get(\"text\", \"\")\n",
    "        safe_text = _safe_text(text)\n",
    "        pairs.append((query, safe_text))\n",
    "        index_map.append(i)\n",
    "\n",
    "    # Batch-predict scores to avoid OOM\n",
    "    rerank_scores = []\n",
    "    for i in tqdm(range(0, len(pairs), RERANKER_BATCH_SIZE), desc=\"Reranker batches\"):\n",
    "        batch = pairs[i : i + RERANKER_BATCH_SIZE]\n",
    "        try:\n",
    "            batch_scores = reranker.predict(batch, show_progress_bar=False)\n",
    "        except TypeError:\n",
    "            # Some cross-encoder versions expect list[str] instead of list[tuple]\n",
    "            # convert to \"query \\t text\" fallback\n",
    "            batch_joined = [q + \"\\t\" + t for q, t in batch]\n",
    "            batch_scores = reranker.predict(batch_joined, show_progress_bar=False)\n",
    "        # ensure list of floats\n",
    "        batch_scores = np.asarray(batch_scores, dtype=float).tolist()\n",
    "        rerank_scores.extend(batch_scores)\n",
    "\n",
    "    # Attach reranker scores back to original results and compute final score\n",
    "    reranked = []\n",
    "    for pair_idx, score in enumerate(rerank_scores):\n",
    "        res_idx = index_map[pair_idx]\n",
    "        orig = results[res_idx].copy()\n",
    "        orig_score = float(orig.get(\"score\", 0.0))\n",
    "        orig[\"rerank_score\"] = float(score)\n",
    "        orig[\"final_score\"] = float(ALPHA * orig[\"rerank_score\"] + (1.0 - ALPHA) * orig_score)\n",
    "        reranked.append(orig)\n",
    "\n",
    "    # Sort by final_score descending and return top_n\n",
    "    reranked_sorted = sorted(reranked, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    return reranked_sorted[:top_n]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage (run after hybrid_retrieval_pipeline)\n",
    "# ---------------------------\n",
    "query = \"What does 6G offer?\"\n",
    "try:\n",
    "    reranked_results = rerank_results(query, hybrid_results, top_n=5)\n",
    "    print(f\"Top {len(reranked_results)} reranked results:\")\n",
    "    for i, r in enumerate(reranked_results, 1):\n",
    "        print(f\"\\n{i}. chunk_id={r.get('chunk_id', 'n/a')} source_type={r.get('source_type','?')}\")\n",
    "        print(f\"   final_score={r['final_score']:.4f} rerank_score={r['rerank_score']:.4f} orig_score={r.get('score', 0.0):.4f}\")\n",
    "        print(\"   snippet:\", r.get(\"text\",\"\")[:300].replace(\"\\n\",\" \"), \"...\")\n",
    "except Exception as e:\n",
    "    print(\"Reranking failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Mini evaluation dataset\n",
    "# ------------------------\n",
    "# Format: {\"query\": \"...\", \"gold_keywords\": [\"word1\", \"word2\", ...]}\n",
    "\n",
    "evaluation_set = [\n",
    "    {\n",
    "        \"query\": \"What does 6G offer?\",\n",
    "        \"gold_keywords\": [\"sub-thz\", \"terahertz\", \"ai-native\", \"ai native\", \n",
    " \"extreme bandwidth\", \"ultra-reliable\", \"low-latency\",\n",
    " \"massive connectivity\", \"holographic\", \"ris\", \"b5g\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is RIS technology?\",\n",
    "        \"gold_keywords\": [\"reconfigurable intelligent surface\", \"ris\", \n",
    " \"metasurface\", \"reflective element\", \"beamforming\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What challenges does 5G face?\",\n",
    "        \"gold_keywords\": [\"latency\", \"energy efficiency\", \"spectrum\", \n",
    " \"massive iot\", \"ultra reliable\", \"coverage\"]\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(evaluation_set)} evaluation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keyword_f1(text: str, keywords: list) -> float:\n",
    "    \"\"\"\n",
    "    Computes F1 between gold keywords and text content.\n",
    "    This version is correct: F1 is always between 0 and 1.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Count hits\n",
    "    hits = sum(1 for k in keywords if k.lower() in text_lower)\n",
    "    total = len(keywords)\n",
    "\n",
    "    # Precision = hits / retrieved_keywords (1 chunk → retrieved_keywords=hits>0 means 1 relevant retrieved)\n",
    "    precision = hits / max(hits, 1)\n",
    "\n",
    "    # Recall = hits / total gold keywords\n",
    "    recall = hits / total if total > 0 else 0.0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_pipeline(evaluation_set, top_n=5):\n",
    "    \"\"\"\n",
    "    Runs a small benchmark:\n",
    "      1) semantic retrieval\n",
    "      2) hybrid retrieval\n",
    "      3) hybrid + reranking\n",
    "    Computes keyword F1 for top retrieved chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sample in evaluation_set:\n",
    "        query = sample[\"query\"]\n",
    "        gold = sample[\"gold_keywords\"]\n",
    "\n",
    "        print(f\"\\n=== Evaluating: {query} ===\")\n",
    "\n",
    "        # ---- 1. Semantic ----\n",
    "        semantic = semantic_retrieval(query, top_k=top_n)\n",
    "        semantic_top = semantic[0][\"text\"] if semantic else \"\"\n",
    "        f1_sem = compute_keyword_f1(semantic_top, gold)\n",
    "        print(f\"Semantic F1: {f1_sem:.3f}\")\n",
    "\n",
    "        # ---- 2. Hybrid ----\n",
    "        hybrid = hybrid_retrieval_pipeline(query, top_k_semantic=top_n)\n",
    "        hybrid_top = hybrid[0][\"text\"] if hybrid else \"\"\n",
    "        f1_hybrid = compute_keyword_f1(hybrid_top, gold)\n",
    "        print(f\"Hybrid F1:   {f1_hybrid:.3f}\")\n",
    "\n",
    "        # ---- 3. Reranked ----\n",
    "        reranked = rerank_results(query, hybrid, top_n=top_n)\n",
    "        reranked_top = reranked[0][\"text\"] if reranked else \"\"\n",
    "        f1_rerank = compute_keyword_f1(reranked_top, gold)\n",
    "        print(f\"Reranked F1: {f1_rerank:.3f}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"semantic_f1\": f1_sem,\n",
    "            \"hybrid_f1\": f1_hybrid,\n",
    "            \"reranked_f1\": f1_rerank\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699b91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating: What does 6G offer? ===\n",
      "Semantic F1: 0.167\n",
      "Hybrid F1:   0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranker batches: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked F1: 0.000\n",
      "\n",
      "=== Evaluating: What is RIS technology? ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic F1: 0.000\n",
      "Hybrid F1:   0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranker batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked F1: 0.000\n",
      "\n",
      "=== Evaluating: What challenges does 5G face? ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic F1: 0.500\n",
      "Hybrid F1:   0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranker batches: 100%|██████████| 1/1 [00:00<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked F1: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'What does 6G offer?',\n",
       "  'semantic_f1': 0.16666666666666669,\n",
       "  'hybrid_f1': 0.16666666666666669,\n",
       "  'reranked_f1': 0.0},\n",
       " {'query': 'What is RIS technology?',\n",
       "  'semantic_f1': 0.0,\n",
       "  'hybrid_f1': 0.0,\n",
       "  'reranked_f1': 0.0},\n",
       " {'query': 'What challenges does 5G face?',\n",
       "  'semantic_f1': 0.5,\n",
       "  'hybrid_f1': 0.5,\n",
       "  'reranked_f1': 0.5}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_pipeline(evaluation_set, top_n=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ded763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-thz                        → not found\n",
      "terahertz                      → not found\n",
      "ai-native                      → not found\n",
      "ai native                      → not found\n",
      "extreme bandwidth              → not found\n",
      "ultra reliable                 → not found\n",
      "low latency                    → not found\n",
      "massive connectivity           → not found\n",
      "holographic                    → not found\n",
      "ris                            → FOUND\n",
      "b5g                            → not found\n"
     ]
    }
   ],
   "source": [
    "def debug_keywords(text, keywords):\n",
    "    text_lower = text.lower()\n",
    "    for k in keywords:\n",
    "        print(f\"{k:30} → {'FOUND' if k.lower() in text_lower else 'not found'}\")\n",
    "\n",
    "debug_keywords(hybrid_results[0][\"text\"], evaluation_set[0][\"gold_keywords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee421f98",
   "metadata": {},
   "source": [
    "# 5. Final Context + Citation Packaging\n",
    "In this last cell we take the relevance-boosted results (semantic reranking) and out put a context dic for LLM to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c4d9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Context Block Ready\n",
      "\n",
      ". In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requires networks that guarantee worldwide digital inclusion to support a wide range of elements, end-to-end (E 2 E) life-cycle tracking to reduce waste and automate recycling, resource-efficient connected agriculture, universal access to digital healthcare, etc. This requires embedded autonomous sensors and actuators, worldwide coverage with outstanding energy-, material-, and cost-efficiency, as well as a network platform with high availability and security [3]. III\n",
      "\n",
      "---\n",
      "\n",
      ". The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged. This will enable seamless movement in a cyberphysical continuum of a connected physi\n",
      "\n",
      "Citations:\n",
      "- ./data/test.pdf | . In addition, the vision of 6 G is to also create more humanfriendly, sustainable, and efficient communities. This requ...\n",
      "- ./data/test.pdf | . The 6 G vision is to create a seamless reality where the physical and digital worlds, so far separated, are converged....\n",
      "- ./data/test.pdf | . To this end, the connected sustainable world empowered by 6 G will require 3 1) resource efficient ICT, through energy...\n",
      "- ./data/test.pdf | . II and overviewing the use cases (UC) that are expected to drive a digital and societal revolution in Sec. III. This i...\n",
      "- ./data/test.pdf | . This article presented a novel perspective on the 6 G vision and the evolution of mobile networks towards this vision....\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Cell 5 (Fixed): Final Context + Citations Packaging (BGE-M3)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_and_deduplicate_structured(items: list):\n",
    "    \"\"\"\n",
    "    Deduplicate based on cleaned text, but keep alignment\n",
    "    between text, source, rerank score, etc.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "\n",
    "    for item in items:\n",
    "        text = item[\"text\"]\n",
    "        key = re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(item)\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def assemble_context(reranked_results: list, max_chars: int = 8000):\n",
    "    \"\"\"\n",
    "    Merge reranked chunks into final LLM context.\n",
    "    Now correctly handles deduplication while preserving metadata alignment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Deduplicate the full items, not just the text\n",
    "    unique_items = clean_and_deduplicate_structured(reranked_results)\n",
    "\n",
    "    merged_text = \"\"\n",
    "    citations = []\n",
    "\n",
    "    for item in unique_items:\n",
    "        text = item[\"text\"].strip()\n",
    "\n",
    "        # Stop once we exceed context budget\n",
    "        if len(merged_text) + len(text) > max_chars:\n",
    "            break\n",
    "\n",
    "        merged_text += text + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        citations.append({\n",
    "            \"source\": item[\"source\"],\n",
    "            \"chunk_preview\": text[:120] + \"...\",\n",
    "            \"rerank_score\": item.get(\"rerank_score\"),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"merged_context\": merged_text.strip(),\n",
    "        \"citations\": citations,\n",
    "        \"raw_chunks\": [item[\"text\"] for item in unique_items]\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Test final context assembly\n",
    "# ---------------------------\n",
    "context_for_llm = assemble_context(reranked_results, max_chars=8000)\n",
    "\n",
    "print(\"Final Context Block Ready\\n\")\n",
    "print(context_for_llm[\"merged_context\"][:800])  # preview first 800 chars\n",
    "print(\"\\nCitations:\")\n",
    "for c in context_for_llm[\"citations\"]:\n",
    "    print(f\"- {c['source']} | {c['chunk_preview']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"context_for_llm exists:\", 'context_for_llm' in locals())\n",
    "\n",
    "if 'context_for_llm' in locals():\n",
    "    print(context_for_llm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9a29c",
   "metadata": {},
   "source": [
    "# Notebook 4\n",
    "From this point on this is what used to be notebook 04 and due to kernel and .venv issues, had to be moved here. Will probably moved back once a solution is found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0c459-a13f-4d0c-9c51-b1b2a701e7d6",
   "metadata": {},
   "source": [
    "# LLM Answering Pipeline\n",
    "This section is responsible for:\n",
    "\n",
    "* Running a local, open-source LLM\n",
    "* Constructing the final prompt (query + context)\n",
    "* Generating an answer\n",
    "* Attaching citations\n",
    "* Returning a polished response\n",
    "\n",
    "Implementing open-source software, we will use:\n",
    "\n",
    "Ollama (best, simplest, FREE local LLM runner)\n",
    "\n",
    "Model: llama3 (this can changed eventually in order to improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cfe7cb",
   "metadata": {},
   "source": [
    "# NB04.1 Imports + Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794d052-d840-4702-8e9a-aa28926e5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Notebook 04 — Cell 3\n",
    "# Imports + Global Settings\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Optional: pretty printing\n",
    "import textwrap\n",
    "\n",
    "# Load Ollama local LLM client (try the official package first, fall back to HTTP)\n",
    "# Preferred: `pip install ollama`\n",
    "llm = None\n",
    "try:\n",
    "    from ollama import Client as OllamaClient\n",
    "    llm = OllamaClient(host=\"http://localhost:11434\")\n",
    "    print(\"Local LLM client ready (ollama package).\")\n",
    "except Exception as e:\n",
    "    # Fallback: lightweight HTTP client using requests\n",
    "    try:\n",
    "        import requests\n",
    "        from requests.exceptions import HTTPError, RequestException\n",
    "\n",
    "        class SimpleOllamaClient:\n",
    "            def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "                self.host = host.rstrip(\"/\")\n",
    "\n",
    "            def _normalize(self, data):\n",
    "                # Normalize common response shapes to a consistent dict\n",
    "                # Ollama's primary shape: {\"response\": \"...\", \"model\": \"...\", ...}\n",
    "                if isinstance(data, dict):\n",
    "                    # Check for Ollama's native \"response\" key first\n",
    "                    if \"response\" in data:\n",
    "                        return {\"message\": {\"content\": data[\"response\"]}}\n",
    "                    # Check for standard message.content shape\n",
    "                    if \"message\" in data and isinstance(data[\"message\"], dict) and \"content\" in data[\"message\"]:\n",
    "                        return data\n",
    "                    # Check for simple text key\n",
    "                    if \"text\" in data:\n",
    "                        return {\"message\": {\"content\": data[\"text\"]}}\n",
    "                    # Check for choices array (OpenAI-like)\n",
    "                    if \"choices\" in data and isinstance(data[\"choices\"], list) and len(data[\"choices\"]) > 0:\n",
    "                        first = data[\"choices\"][0]\n",
    "                        if isinstance(first, dict) and (\"text\" in first or \"message\" in first):\n",
    "                            text = first.get(\"text\") or (first.get(\"message\") if isinstance(first.get(\"message\"), str) else (first.get(\"message\", {}).get(\"content\") if isinstance(first.get(\"message\"), dict) else None))\n",
    "                            return {\"message\": {\"content\": text}}\n",
    "                # Fallback: stringify\n",
    "                return {\"message\": {\"content\": str(data)}}\n",
    "\n",
    "            def chat(self, model: str, messages: list):\n",
    "                \"\"\"Send a consolidated prompt to Ollama HTTP API and return a normalized response.\n",
    "\n",
    "                Returns a dict containing `{\"message\": {\"content\": <str>}}` on success.\n",
    "                \"\"\"\n",
    "                prompt_text = \"\\n\".join([m.get(\"content\", \"\") for m in messages if m.get(\"role\") == \"user\"]).strip()\n",
    "\n",
    "                url = f\"{self.host}/api/generate\"\n",
    "                payload = {\"model\": model, \"prompt\": prompt_text, \"stream\": False}\n",
    "\n",
    "                try:\n",
    "                    resp = requests.post(url, json=payload, timeout=60)\n",
    "                    resp.raise_for_status()\n",
    "                except HTTPError as he:\n",
    "                    status = getattr(he.response, \"status_code\", None)\n",
    "                    if status == 404:\n",
    "                        raise RuntimeError(\n",
    "                            f\"Ollama HTTP endpoint not found (404). Is the Ollama daemon running at {self.host}?\"\n",
    "                        ) from he\n",
    "                    raise\n",
    "                except RequestException as re:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Failed to reach Ollama at {self.host}: {re}. Ensure the daemon is running and reachable.\"\n",
    "                    ) from re\n",
    "\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except ValueError:\n",
    "                    # Non-JSON response\n",
    "                    return {\"message\": {\"content\": resp.text}}\n",
    "\n",
    "                return self._normalize(data)\n",
    "\n",
    "        llm = SimpleOllamaClient(host=\"http://localhost:11434\")\n",
    "        print(\"Using fallback HTTP Ollama client (requests).\")\n",
    "    except Exception as e2:\n",
    "        print(\"Could not import `ollama` package or use HTTP fallback.\")\n",
    "        print(\"To enable the local LLM client, either:\")\n",
    "        print(\"  1) pip install ollama\")\n",
    "        print(\"  2) ensure Ollama daemon is running at http://localhost:11434 and install requests (`pip install requests`)\")\n",
    "        print(\"Falling back to a stub llm object that raises if used.\")\n",
    "\n",
    "        class _StubLLM:\n",
    "            def chat(self, *args, **kwargs):\n",
    "                raise RuntimeError(\"No Ollama client available. Install `ollama` or `requests` and start the Ollama daemon.\")\n",
    "\n",
    "        llm = _StubLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7e887-7abe-4c11-9d50-efd02b9df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Notebook 04 — Cell 2\n",
    "# Prompt Template + Formatting Function\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def build_rag_prompt(query: str, context_data: Dict):\n",
    "    \"\"\"\n",
    "    Build the final RAG prompt to send to the local LLM.\n",
    "    Includes:\n",
    "    - merged context\n",
    "    - user question\n",
    "    - instruction to use citations\n",
    "    \"\"\"\n",
    "\n",
    "    context_block = context_data[\"merged_context\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant answering questions using ONLY the context provided.\n",
    "\n",
    "CONTEXT:\n",
    "-------\n",
    "{context_block}\n",
    "-------\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use ONLY the information in the context.\n",
    "- Do NOT hallucinate.\n",
    "- Cite your sources using this format: [source].\n",
    "- If the answer is not in the context, say: \"The answer is not available in the provided context.\"\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "FINAL ANSWER (with citations):\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e302831-91a0-4a6d-9f26-086f3ae51488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Notebook 04 — Cell 3\n",
    "# Generate Answer using Local LLM (Ollama)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def generate_llm_answer(query: str, context_for_llm: Dict, model: str = None):\n",
    "    \"\"\"\n",
    "    Sends the final prompt to the local LLM and retrieves the response.\n",
    "    If model is not specified, uses OLLAMA_MODEL env var (default: llama3.1:latest).\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:latest\")\n",
    "    \n",
    "    prompt = build_rag_prompt(query, context_for_llm)\n",
    "\n",
    "    try:\n",
    "        response = llm.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        # Re-raise runtime errors from the client (e.g., helpful 404 message)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\") from e\n",
    "\n",
    "    # Attempt to extract a text response from a range of possible shapes\n",
    "    content = None\n",
    "    if isinstance(response, dict):\n",
    "        # Check for normalized message.content first (most common after _normalize)\n",
    "        if \"message\" in response and isinstance(response[\"message\"], dict) and \"content\" in response[\"message\"]:\n",
    "            content = response[\"message\"][\"content\"]\n",
    "        # Fallback checks in case _normalize wasn't applied\n",
    "        elif \"response\" in response:\n",
    "            content = response[\"response\"]\n",
    "        elif \"text\" in response:\n",
    "            content = response[\"text\"]\n",
    "        elif \"choices\" in response and isinstance(response[\"choices\"], list) and len(response[\"choices\"]) > 0:\n",
    "            first = response[\"choices\"][0]\n",
    "            if isinstance(first, dict):\n",
    "                content = first.get(\"text\") or (first.get(\"message\") if isinstance(first.get(\"message\"), str) else (first.get(\"message\", {}).get(\"content\") if isinstance(first.get(\"message\"), dict) else None))\n",
    "\n",
    "    if content is None:\n",
    "        # Fallback: stringify whatever we got\n",
    "        content = str(response)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# ---- Test the answering pipeline (only if variables are defined) ----\n",
    "# If 'context_for_llm' comes from notebook 03, use it. Otherwise, use a sample query.\n",
    "if 'context_for_llm' in locals():\n",
    "    # Use the query and context from notebook 03\n",
    "    sample_answer = generate_llm_answer(query, context_for_llm)\n",
    "    \n",
    "    print(\"\\n===== LLM ANSWER =====\\n\")\n",
    "    print(sample_answer)\n",
    "else:\n",
    "    print(\"Variable 'context_for_llm' not found in kernel.\")\n",
    "    print(\"Please run Notebook 03 first to generate the retrieval context, then return here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703fd922-c0f3-46b2-890a-1d2431beb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Notebook 04 — Cell 4\n",
    "# Final Output Formatting (Answer + Citations)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def format_final_output(answer: str, context_for_llm: Dict):\n",
    "    \"\"\"\n",
    "    Returns a clean, structured output including:\n",
    "    - Answer\n",
    "    - Citations\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(\"=======================\\n\")\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"CITATIONS\")\n",
    "    print(\"=======================\\n\")\n",
    "    for c in context_for_llm[\"citations\"]:\n",
    "        print(f\"- Source: {c['source']}\")\n",
    "        print(f\"  Preview: {c['chunk_preview']}\")\n",
    "        print(f\"  Score: {c['rerank_score']:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Display final result (only if variables are defined)\n",
    "if 'sample_answer' in locals() and 'context_for_llm' in locals():\n",
    "    format_final_output(sample_answer, context_for_llm)\n",
    "else:\n",
    "    print(\"Variables 'sample_answer' and/or 'context_for_llm' not found.\")\n",
    "    print(\"Please run cells above first to generate these variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6d684-4dd3-4867-98ed-bc5046389490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Ollama HTTP diagnostic (run this cell to test your local Ollama docker)\n",
    "import os\n",
    "print(\"--- Ollama HTTP Diagnostic ---\")\n",
    "host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "model = os.getenv('OLLAMA_MODEL', 'llama3.1:latest')\n",
    "print(f'Using host={host} model={model}')\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print('requests version:', requests.__version__)\n",
    "except Exception as e:\n",
    "    print('requests not available in this kernel:', e)\n",
    "\n",
    "# Quick GET checks for common endpoints\n",
    "for path in ['', '/api', '/api/models', '/api/generate']:\n",
    "    url = host.rstrip('/') + path\n",
    "    try:\n",
    "        r = requests.get(url, timeout=4)\n",
    "        print(f'GET {path} ->', r.status_code)\n",
    "    except Exception as e:\n",
    "        print(f'GET {path} error:', e)\n",
    "\n",
    "# Try a small generate POST to /api/generate\n",
    "payload = {'model': model, 'prompt': 'Say hello and identify the model used.', 'stream': False}\n",
    "try:\n",
    "    gen_url = host.rstrip('/') + '/api/generate'\n",
    "    r = requests.post(gen_url, json=payload, timeout=20)\n",
    "    print('POST /api/generate ->', r.status_code)\n",
    "    text = r.text\n",
    "    print('Response (truncated):', text[:800])\n",
    "    try:\n",
    "        j = r.json()\n",
    "        print('JSON keys:', list(j.keys()))\n",
    "        # If llm object exists with _normalize, try to normalize and show result\n",
    "        if 'llm' in globals() and hasattr(llm, '_normalize'):\n",
    "            try:\n",
    "                print('Normalized sample:', llm._normalize(j))\n",
    "            except Exception as e:\n",
    "                print('Normalization error:', e)\n",
    "        else:\n",
    "            # Try simple normalization heuristics\n",
    "            if isinstance(j, dict):\n",
    "                if 'message' in j:\n",
    "                    print('message:', j.get('message'))\n",
    "                elif 'text' in j:\n",
    "                    print('text:', j.get('text'))\n",
    "                elif 'choices' in j and isinstance(j['choices'], list) and len(j['choices'])>0:\n",
    "                    print('choices[0]:', j['choices'][0])\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('POST /api/generate error:', e)\n",
    "\n",
    "print('Diagnostic complete. If you see 404 from POST, verify the Ollama daemon and model name (try OLLAMA_MODEL=llama3.1:latest).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23f487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
