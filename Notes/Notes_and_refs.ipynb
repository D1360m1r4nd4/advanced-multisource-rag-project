{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d55201",
   "metadata": {},
   "source": [
    "# Chunking in RAG systems\n",
    "\n",
    "## 1 Embeddings Context Limits\n",
    "\n",
    "When embedings (vector representation of text) are created, most models have a maximum token limit (usually around 1000 tokens -> 750 words). \n",
    "\n",
    "If text exceeds that limit: \n",
    "* The model either truncates the input (loosing information in the process).\n",
    "* You can't process it at all.\n",
    "So splitting ensures every piece fits within the range of what the embedding model can handle.\n",
    "\n",
    "## 2 Better Semantic Accuracy\n",
    "\n",
    "Embeddings capture the meaning of a text chunk as a single vector in high-dimensional space.\n",
    "\n",
    "If a chunk is too large:\n",
    "\n",
    "* It mixes multiple topics or sections together.\n",
    "* The embedding becomes less precise - it represents an average of different ideas.\n",
    "\n",
    "If a chung is too small:\n",
    "* It may lack context and produce embeddings that are too narrow or meaningless.\n",
    "\n",
    "That's why most RAG systems target the range between 200-1000 tokens.\n",
    "\n",
    "## 3 Efficient Retrieval\n",
    "\n",
    "When a user aks a question in a RAG system: \n",
    "\n",
    "1. The system converts the question into an embedding vector. \n",
    "2. It retrieves the most similar document chunk (based on cosine similarity). \n",
    "3. It sends those relevant chunks to the LLM to generate the answer.\n",
    "\n",
    "If you don't chunk: \n",
    "* You'd have to embed massive documents - which is slow and expensive.\n",
    "* Retrieval would be less focused - the model might pull irrelevant sections.\n",
    "\n",
    "## 4 Easier Updates and Maintenance\n",
    "Smaller chunks mean:\n",
    "\n",
    "* You can re-embed or update only affected sections (not the entire document).\n",
    "* You can parallelize embedding jobs easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f8b3e",
   "metadata": {},
   "source": [
    "# Upgrade Embeddings to BGE-M3\n",
    "\n",
    "## Why BGE-M3 Is Better (and Why It Matters for Your RAG Project)\n",
    "\n",
    "BGE-M3 is one of the newest embedding models from BAAI (the same org that created BGE-Large, one of the most popular RAG embedding models of 2023â€“2024).\n",
    "\n",
    "But unlike classic embedding models that perform only dense retrieval, BGE-M3 does three things at once â€” and this is why itâ€™s so powerful.\n",
    "\n",
    "### 1. Multi-Vector Retrieval (RAG-superpower)\n",
    "\n",
    "    Most embedding models output one single dense vector per chunk.\n",
    "\n",
    "    BGE-M3 can output multiple vectors per chunk (multi-vector embeddings).\n",
    "\n",
    "Why does this help?\n",
    "\n",
    "* **Better handling of long, complex chunks**\n",
    "\n",
    "If a chunk has several topics or facts, one dense vector can only encode a â€œblurâ€ of all of them.\n",
    "\n",
    "Multi-vector embeddings allow the model to represent:\n",
    "\n",
    "    concept A with vector #1\n",
    "\n",
    "    concept B with vector #2\n",
    "\n",
    "    concept C with vector #3\n",
    "    \n",
    "    â€¦all inside the same chunk.\n",
    "\n",
    "This makes search performance shoot up for real-world documents like:\n",
    "\n",
    "* reports\n",
    "\n",
    "* regulations\n",
    " \n",
    "* PDFs\n",
    " \n",
    "* long paragraphs\n",
    " \n",
    "* mixed-topic chunks\n",
    "\n",
    "This is especially good when paired with long-document scenario.\n",
    "\n",
    "### 2. Multi-Lingual\n",
    "\n",
    "Projects and workflows already mixes English + Spanish â€” sometimes even French.\n",
    "\n",
    "BGE-M3 was designed to be truly multilingual.\n",
    "\n",
    "It handles:\n",
    "\n",
    "* English\n",
    " \n",
    "* Spanish\n",
    " \n",
    "* French\n",
    " \n",
    "* German\n",
    " \n",
    "* Chinese\n",
    "\n",
    "* and more\n",
    "\n",
    "The embeddings for different languages live in a shared vector space, so queries in English can retrieve Spanish chunks accurately.\n",
    "\n",
    "This is something many older models (like all-MiniLM-L6-v2) do poorly.\n",
    "\n",
    "### 3. Multi-Granularity (short queries â‡† long queries)\n",
    "\n",
    "Older embedding models struggle with short vs long queries:\n",
    "\n",
    "Short queries â†’ too ambiguous, poor retrieval\n",
    "\n",
    "Long queries â†’ not enough context\n",
    "\n",
    "BGE-M3 processes:\n",
    "\n",
    "* keywords\n",
    "\n",
    "* questions\n",
    "\n",
    "* long semantic queries\n",
    "\n",
    "all consistently.\n",
    "\n",
    "This is a big deal for RAG QA systems, especially when you have both:\n",
    "\n",
    "* keyword-style queries (â€œsales France 2023â€)\n",
    "\n",
    "* natural language queries (â€œWhat were the main drivers of sales growth in France?â€)\n",
    "\n",
    "### Summary Table â€” Why BGE-M3 Wins\n",
    "| Feature | Traditional embeddings | **BGE-M3** | \n",
    "| --- | --- | --- | \n",
    "| Multi-lingual | ğŸŸ¡ Medium | ğŸŸ¢ Excellent | \n",
    "| Multi-vector retrieval | ğŸ”´ No | ğŸŸ¢ Yes | \n",
    "| Handles long documents | ğŸŸ¡ Okay | ğŸŸ¢ Excellent | \n",
    "| Handles long queries | ğŸŸ¡ Meh | ğŸŸ¢ Great | \n",
    "| Handles short queries | ğŸŸ¡ Varied | ğŸŸ¢ Consistent | \n",
    "| Retrieval accuracy | ğŸŸ¡ Good | ğŸŸ¢ **SOTA-level** | \n",
    "| Vector DB compatibility | ğŸŸ¢ Yes | ğŸŸ¢ Yes | \n",
    "| Best for RAG? | ğŸŸ¡ Depends | ğŸŸ¢ **Yes** |â€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd11d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
