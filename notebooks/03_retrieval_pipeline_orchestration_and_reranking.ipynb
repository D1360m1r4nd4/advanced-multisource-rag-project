{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b9a18",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline Orchestration + Reranking\n",
    "\n",
    "**Goal**\n",
    "Build a hybrid retrieval system that:\n",
    "\n",
    "1. Pulls candidate results from your Qdrant vector store (semantic retrieval).\n",
    "\n",
    "2. Optionally augments those with graph-based or metadata filters (hierarchical or relational layer).\n",
    "\n",
    "3. Applies cross-encoder reranking to refine relevance before passing context to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d494c",
   "metadata": {},
   "source": [
    "# 1: Setup & Connections\n",
    "\n",
    "* Import required libraries.\n",
    "* Load/connect to Qdrant (vector DB) and Memgraph (graph DB).\n",
    "* Load the embedding model (for encoding queries) and the cross-encoder reranker (for re-scoring * candidate documents).\n",
    "* Define some configuration variables (collection name, top_k defaults).\n",
    "* Print short status messages so you know everything connected/loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb589d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Qdrant at http://localhost:6333. Collections: ['enterprise_docs']\n",
      "âœ… Connected to Memgraph at localhost:7687\n",
      "âœ… Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ… Loaded reranker model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      " --- Summary --- \n",
      "Qdrant client: OK\n",
      "Memgraph client: OK\n",
      "Embedder: OK (sentence-transformers/all-MiniLM-L6-v2)\n",
      "Reranker: OK (cross-encoder/ms-marco-MiniLM-L-6-v2)\n",
      "Qdrant collection (to use): enterprise_docs\n",
      "Retrieval settings: DEFAULT_TOP_K=50, FINAL_TOP_K=5\n"
     ]
    }
   ],
   "source": [
    "# Before running this notebook, ensure you have run Notebook 01 and Notebook 02 to set up the databases and ingest data.\n",
    "# Also, make sure your Qdrant and Memgraph services are running.\n",
    "# Run docker commands from root project @ terminal: \n",
    "# docker-compose up -d\n",
    "# Or, if running individually:\n",
    "# docker run -p 6333:6333 qdrant/qdrant\n",
    "# docker run -it -p 7687:7687 -p 3000:3000 memgraph/memgraph-platform\n",
    "\n",
    "#  Standard libs \n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "#  Utilities \n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Qdrant client (vector DB) \n",
    "# pip package: qdrant-client\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "#  Memgraph client (graph DB) \n",
    "# pip package: gqlalchemy\n",
    "from gqlalchemy import Memgraph\n",
    "\n",
    "#  Sentence Transformers (embeddings + cross-encoder) \n",
    "# pip package: sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "#  Configuration: modify if your services run on different hosts/ports \n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "MEMGRAPH_HOST = os.getenv(\"MEMGRAPH_HOST\", \"localhost\")\n",
    "MEMGRAPH_PORT = int(os.getenv(\"MEMGRAPH_PORT\", 7687))\n",
    "\n",
    "# Name of the Qdrant collection we created in Notebook 02.\n",
    "# Change needed later to allow for multiple collections\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION\", \"enterprise_docs\")\n",
    "\n",
    "# How many candidates to fetch from each retriever before reranking\n",
    "DEFAULT_TOP_K = 50        # number of candidates from vector search\n",
    "FINAL_TOP_K = 5           # final number of results to return to user after rerank\n",
    "\n",
    "#  Connect to Qdrant \n",
    "try:\n",
    "    qdrant_client = QdrantClient(url=QDRANT_URL)\n",
    "    # A small call to verify the collection exists (raises if unreachable)\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"âœ… Connected to Qdrant at {QDRANT_URL}. Collections: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    qdrant_client = None\n",
    "    print(\"âŒ Could not connect to Qdrant:\", e)\n",
    "\n",
    "#  Connect to Memgraph \n",
    "try:\n",
    "    memgraph = Memgraph(host=MEMGRAPH_HOST, port=MEMGRAPH_PORT)\n",
    "    # Simple test query to verify connection (returns a single row)\n",
    "    test = memgraph.execute_and_fetch(\"RETURN 1 AS ok\")\n",
    "    _ = list(test)  # consume generator\n",
    "    print(f\"âœ… Connected to Memgraph at {MEMGRAPH_HOST}:{MEMGRAPH_PORT}\")\n",
    "except Exception as e:\n",
    "    memgraph = None\n",
    "    print(\"âŒ Could not connect to Memgraph:\", e)\n",
    "\n",
    "#  Load embedding model for queries \n",
    "# Use the same model family you used to embed documents in Notebook 02.\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "try:\n",
    "    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(f\"âœ… Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    embedder = None\n",
    "    print(\"âŒ Could not load embedding model:\", e)\n",
    "\n",
    "# Coross-encoder for reranking choices depend largely on available resources and implementation needs.\n",
    "# Load cross-encoder reranker (can be large) \n",
    "# Choose a comparatively small reranker for local usage if resources are limited:\n",
    "# e.g., \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANKER_MODEL_NAME = os.getenv(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "try:\n",
    "    reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
    "    print(f\"âœ… Loaded reranker model: {RERANKER_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    reranker = None\n",
    "    print(\"âš ï¸ Could not load reranker model (this is optional). Error:\", e)\n",
    "\n",
    "#  Quick summary of loaded components \n",
    "print(\"\\n --- Summary --- \")\n",
    "print(f\"Qdrant client: {'OK' if qdrant_client else 'MISSING'}\")\n",
    "print(f\"Memgraph client: {'OK' if memgraph else 'MISSING'}\")\n",
    "print(f\"Embedder: {'OK' if embedder else 'MISSING'} ({EMBEDDING_MODEL_NAME})\")\n",
    "print(f\"Reranker: {'OK' if reranker else 'MISSING'} ({RERANKER_MODEL_NAME})\")\n",
    "print(f\"Qdrant collection (to use): {COLLECTION_NAME}\")\n",
    "print(f\"Retrieval settings: DEFAULT_TOP_K={DEFAULT_TOP_K}, FINAL_TOP_K={FINAL_TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd2eb7",
   "metadata": {},
   "source": [
    "# 2 Semantic Retrieval from Qdrant\n",
    "\n",
    "Here we will:\n",
    "1. Take a natural language query.\n",
    "2. Encode it into a vector using your embedding model.\n",
    "3. Search for the most semantically similar chunks (documents) in Qdrant.\n",
    "4. Return and display the top K results, with metadata for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d02be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Top 5 results for query: 'What are the key features of RIS technology?'\n",
      "\n",
      "Result 1: (score=0.3908)\n",
      "Source: ../data/test.pdf | Page: None\n",
      "Snippet: intent-based management should be deployed to allow human-network interaction [3]. To this end, human operators use high-level declarative languages to define system operational goals in the form of â€intents, which are then translated to lower-level instructions (action plans and settings) using int...\n",
      "\n",
      "Result 2: (score=0.3607)\n",
      "Source: ../data/test.pdf | Page: None\n",
      "Snippet: anyone? The essence of the Io S, coined by Ericsson [4], is built on delivering multisensory experiences over the networks. This UC allows humans to have digital sensory immersive experiences that replicate or even augment what we experience in the physical world through visual, auditory, haptic, ol...\n",
      "\n",
      "Result 3: (score=0.3191)\n",
      "Source: ../data/test.pdf | Page: None\n",
      "Snippet: be needed to facilitate automation, where most of the development and deployment of distributed applications is performed on top of the network infrastructure. In this sense, regardless of the dynamic network changes, the application will always have access to a local computing service. VI. A SPOTLI...\n",
      "\n",
      "Result 4: (score=0.3008)\n",
      "Source: ../data/test.pdf | Page: None\n",
      "Snippet: â€¢Patients' mood and/or health â€¢Band musicians act on sensor s over the are inferred by servers interfaces of their smart for real-time connected tothe Io Must instruments to deliver to corehearsals. network through data located or remote audience mix of those received from patientâ€™s visuals displaye...\n",
      "\n",
      "Result 5: (score=0.2919)\n",
      "Source: ../data/test.pdf | Page: None\n",
      "Snippet: The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (Io T), the unprecedented strain on communication networks put by t...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10448/3143761866.py:17: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "# Define a function to search Qdrant using vector similarity\n",
    "def semantic_retrieval(\n",
    "    query: str,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve the top_k most similar chunks for a given query using cosine similarity.\n",
    "    Returns: List of dictionaries containing metadata + scores.\n",
    "    \"\"\"\n",
    "    # Encode the input query into a vector\n",
    "    query_vector = embedder.encode(query).tolist()\n",
    "\n",
    "    # Search the Qdrant collection\n",
    "    # We use 'search' to get the most similar documents by cosine similarity\n",
    "    # 'with_payload=True' ensures we also retrieve metadata (like source, page, etc.)\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # Parse and format results for easier handling\n",
    "    formatted_results = []\n",
    "    for r in results:\n",
    "        payload = r.payload or {}\n",
    "        formatted_results.append({\n",
    "            \"id\": r.id,\n",
    "            \"score\": r.score,  # similarity score\n",
    "            \"text\": payload.get(\"text\", \"\"),  # the actual content chunk\n",
    "            \"source\": payload.get(\"source\", \"unknown\"),\n",
    "            \"page\": payload.get(\"page\", None),\n",
    "            \"metadata\": payload,\n",
    "        })\n",
    "\n",
    "    return formatted_results\n",
    "\n",
    "# ===========================================\n",
    "# Example Query â€” try one from your dataset\n",
    "# ===========================================\n",
    "# Replace this with a question relevant to your data.\n",
    "sample_query = \"What are the key features of RIS technology?\"\n",
    "\n",
    "# Run the retrieval function\n",
    "retrieved_docs = semantic_retrieval(sample_query, top_k=5)\n",
    "\n",
    "# Display retrieved documents\n",
    "print(f\"\\nðŸ”Ž Top {len(retrieved_docs)} results for query: '{sample_query}'\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i}: (score={doc['score']:.4f})\")\n",
    "    print(f\"Source: {doc['source']} | Page: {doc.get('page')}\")\n",
    "    print(f\"Snippet: {doc['text'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038b188",
   "metadata": {},
   "source": [
    "# 3 Hybrid Retrieval (Vector + Graph)\n",
    "This next cell implements a hybrid retrieval layer that merges: \n",
    "1. **Semantic relevance**\n",
    "2. **Graph-based context**\n",
    "\n",
    "This hybrid approach improves entreprise knowledge retrieval by connecting related entities instead of relying exclusively on text similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c12fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 semantic + 0 graph results.\n",
      "\n",
      " Combined results for query: 'What does 6G offer?'\n",
      "\n",
      "Result 1: [Source: ../data/test.pdf] Score: 0.46595055\n",
      "Snippet: The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories...\n",
      "\n",
      "Result 2: [Source: ../data/test.pdf] Score: 0.4221406\n",
      "Snippet: anticipated 6 G promise into reality. REFERENCES [1] M. Al-Quraan, L. Mohjazi, L. Bariah, A. Centeno, A. Zoha, K. Arshad, K. Assaleh, S. Muhaidat, M. Debbah, and M. A. Imran, â€œEdge-native intelligence...\n",
      "\n",
      "Result 3: [Source: ../data/test.pdf] Score: 0.4129762\n",
      "Snippet: M.A.Imranarewiththe James Watt Schoolof Engineering, University of Glasgow, Glasgow, G 12 8 QQ, UK. (e-mail:{Lina. Mohjazi, Muhammad.Imran}@glasgow.ac.uk). B. Selim is with EÂ´cole de Technologie SupeÂ´...\n",
      "\n",
      "Result 4: [Source: ../data/test.pdf] Score: 0.32941002\n",
      "Snippet: e.g., mobile user equipment connected to different networks or base stations, or a human seeking medical care in different locations. F. Distribution Recent advancements in virtualization of radio acc...\n",
      "\n",
      "Result 5: [Source: ../data/test.pdf] Score: 0.31963313\n",
      "Snippet: 3) ICT for inclusion, that provides equal digital opportunities and helps support education and digital literacy; 4) ICT for society, with resilient networks supporting the economic development and pr...\n",
      "\n",
      "Result 6: [Source: ../data/test.pdf] Score: 0.30803752\n",
      "Snippet: deployment of very dense networks as well as to ensure the strict carbon neutrality targets discussed earlier. To this end, dynamic and flexible deployment solutions will become a cornerstone of 6 G n...\n",
      "\n",
      "Result 7: [Source: ../data/test.pdf] Score: 0.245388\n",
      "Snippet: from pre-cloud legacy architectures [13]. Therefore, both RAN and core service-based architectures will need to be re-engineered to include service-based interfaces that enable higher levels of optimi...\n",
      "\n",
      "Result 8: [Source: ../data/test.pdf] Score: 0.22947413\n",
      "Snippet: be needed to facilitate automation, where most of the development and deployment of distributed applications is performed on top of the network infrastructure. In this sense, regardless of the dynamic...\n",
      "\n",
      "Result 9: [Source: ../data/test.pdf] Score: 0.22291335\n",
      "Snippet: 6 G Architectural Pillars. which can be of diverse and new generations of modalities, leading the data processing to be highly distributed in tiers of different capabilities and include inferencing, m...\n",
      "\n",
      "Result 10: [Source: MOCK_DATA.csv] Score: 0.18712772\n",
      "Snippet: id: 97, first_name: Geri, last_name: Sweeten, email: gsweeten 2 o@blogtalkradio.com, gender: Female, ip_address: 161.231.190.134, country: Mexico, role: Marketing Assistant...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10448/3143761866.py:17: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "def graph_retrieval(\n",
    "    query: str,\n",
    "    memgraph_conn: Memgraph,\n",
    "    limit: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve additional context from Memgraph based on related nodes or edges.\n",
    "    You can customize the Cypher query for your use case.\n",
    "    Example: retrieve nodes whose 'name' or 'description' matches the query.\n",
    "    \"\"\"\n",
    "    if memgraph_conn is None:\n",
    "        print(\" Memgraph not connected â€” skipping graph retrieval.\")\n",
    "        return []\n",
    "\n",
    "    # Simple Cypher example \n",
    "    # This assumes your graph has nodes with a 'name' or 'text' property.\n",
    "    # The cypher_query can be modified to suit the context of Memgraph schema.\n",
    "    cypher_query = f\"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE toLower(n.name) CONTAINS toLower(\"{query}\")\n",
    "       OR toLower(n.text) CONTAINS toLower(\"{query}\")\n",
    "    RETURN n.name AS name, n.text AS text, labels(n) AS labels\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        results = list(memgraph_conn.execute_and_fetch(cypher_query))\n",
    "        # Normalize output to consistent dict structure\n",
    "        return [\n",
    "            {\n",
    "                \"source\": \"memgraph\",\n",
    "                \"text\": r.get(\"text\", r.get(\"name\", \"\")),\n",
    "                \"labels\": r.get(\"labels\", []),\n",
    "                \"score\": 0.5,  # neutral base score; reranker will adjust later\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(\"Graph retrieval failed:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def hybrid_retrieval_pipeline(query: str, top_k_semantic: int = DEFAULT_TOP_K) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1. Retrieve semantic candidates from Qdrant.\n",
    "    2. Retrieve graph-based context from Memgraph.\n",
    "    3. Merge, deduplicate, and prepare for reranking.\n",
    "    \"\"\"\n",
    "    #  Step 1: Vector-based retrieval\n",
    "    semantic_results = semantic_retrieval(query, top_k=top_k_semantic)\n",
    "\n",
    "    #  Step 2: Graph-based retrieval \n",
    "    graph_results = graph_retrieval(query, memgraph_conn=memgraph)\n",
    "\n",
    "    #  Step 3: Merge results \n",
    "    # Simply concatenate lists for now; deduplication can be added if overlap exists\n",
    "    combined_results = semantic_results + graph_results\n",
    "\n",
    "    print(f\"Retrieved {len(semantic_results)} semantic + {len(graph_results)} graph results.\")\n",
    "    return combined_results\n",
    "\n",
    "\n",
    "# Example hybrid query\n",
    "# ===========================================\n",
    "sample_query = \"What does 6G offer?\"\n",
    "\n",
    "hybrid_results = hybrid_retrieval_pipeline(sample_query, top_k_semantic=10)\n",
    "\n",
    "# Preview results\n",
    "print(f\"\\n Combined results for query: '{sample_query}'\\n\")\n",
    "for i, doc in enumerate(hybrid_results[:10], start=1):\n",
    "    print(f\"Result {i}: [Source: {doc.get('source', 'unknown')}] Score: {doc.get('score', 'N/A')}\")\n",
    "    print(f\"Snippet: {doc.get('text', '')[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd92a9e",
   "metadata": {},
   "source": [
    "# 4 Semantic Reranking\n",
    "\n",
    "(after hybrid retrieval)\n",
    "\n",
    "This cell takes everything retrieved in Cell 3, and reorders the results using a cross-encoder, which computes true, pairwise relevance between the query and each candidate chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df735890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked 1 â€” Score: 0.0978\n",
      "Source: ../data/test.pdf\n",
      "Text snippet: The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (Io T), the unpre...\n",
      "--------------------------------------------------------------------------------\n",
      "Reranked 2 â€” Score: -1.5797\n",
      "Source: ../data/test.pdf\n",
      "Text snippet: anticipated 6 G promise into reality. REFERENCES [1] M. Al-Quraan, L. Mohjazi, L. Bariah, A. Centeno, A. Zoha, K. Arshad, K. Assaleh, S. Muhaidat, M. Debbah, and M. A. Imran, â€œEdge-native intelligence for 6 G communications driven by federated learni...\n",
      "--------------------------------------------------------------------------------\n",
      "Reranked 3 â€” Score: -2.8368\n",
      "Source: ../data/test.pdf\n",
      "Text snippet: 6 G Architectural Pillars. which can be of diverse and new generations of modalities, leading the data processing to be highly distributed in tiers of different capabilities and include inferencing, machine learning (ML), sensor fusion, and sensing-a...\n",
      "--------------------------------------------------------------------------------\n",
      "Reranked 4 â€” Score: -3.0998\n",
      "Source: ../data/test.pdf\n",
      "Text snippet: 3) ICT for inclusion, that provides equal digital opportunities and helps support education and digital literacy; 4) ICT for society, with resilient networks supporting the economic development and productivity; 5) ICT for the environment, facilitati...\n",
      "--------------------------------------------------------------------------------\n",
      "Reranked 5 â€” Score: -3.6346\n",
      "Source: ../data/test.pdf\n",
      "Text snippet: deployment of very dense networks as well as to ensure the strict carbon neutrality targets discussed earlier. To this end, dynamic and flexible deployment solutions will become a cornerstone of 6 G networks. This is essential for ensuring future-pro...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Expected Inputs:\n",
    "# - `query` (string): the user's search query\n",
    "# - `hybrid_results` (list): results returned from Cell 3 (vector + graph)\n",
    "#\n",
    "# Output:\n",
    "# - In-order, relevance-boosted results using a cross-encoder\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a lightweight, fast, open-source reranker\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANKER_MODEL)\n",
    "\n",
    "def rerank_results(query: str, results: list, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Apply cross-encoder reranking to hybrid-retrieved results.\n",
    "    \n",
    "    The cross encoder evaluates each (query, passage) pair directly.\n",
    "    This usually gives 20â€“40% better top-1 accuracy than embeddings alone.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build (query, text) pairs for model inference\n",
    "    pairs = [(query, r[\"text\"]) for r in results]\n",
    "    \n",
    "    # Predict semantic similarity scores\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Attach new scores\n",
    "    for r, s in zip(results, scores):\n",
    "        r[\"rerank_score\"] = float(s)\n",
    "    \n",
    "    # Sort descending by score\n",
    "    reranked = sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    \n",
    "    return reranked[:top_n]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ðŸ” Try it out\n",
    "# ---------------------------\n",
    "query = \"What does 6G offer?\"\n",
    "reranked = rerank_results(query, hybrid_results, top_n=5)\n",
    "\n",
    "# Pretty print\n",
    "for i, item in enumerate(reranked, 1):\n",
    "    print(f\"Reranked {i} â€” Score: {item['rerank_score']:.4f}\")\n",
    "    print(f\"Source: {item['source']}\")\n",
    "    print(f\"Text snippet: {item['text'][:250]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4d9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Final Context Block Ready\n",
      "\n",
      "The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is bringing an innovative fabric of breakthrough technologies, enabling smart factories, cities, and Internet-of-Things (Io T), the unprecedented strain on communication networks put by these applications, in terms of highly cognitive, agile architectures and the support of massive connectivity, energy efficiency, and extreme ultralow latency, is pushing 5 G to their limits. As such, the focus of academic and industrial efforts has shifted toward beyond 5 G (B 5 G) and the conceptualization of sixth generation (6 G) systems. This article discusses four main digital and societal use cases (UCs) that will drive the need to reconcile a new breed of network requirements. Based on th\n",
      "\n",
      "ðŸ”— Citations:\n",
      "- ../data/test.pdf | The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is brin...\n",
      "- ../data/test.pdf | anticipated 6 G promise into reality. REFERENCES [1] M. Al-Quraan, L. Mohjazi, L. Bariah, A. Centeno, A. Zoha, K. Arshad...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# ðŸ“¦ Cell 5: Build Final Context + Citation Packaging\n",
    "# -------------------------------------------------------------\n",
    "# Input: `reranked` â†’ output from Cell 4\n",
    "# Output: `context_for_llm` â†’ dictionary containing:\n",
    "#   - merged_context (clean, deduped text)\n",
    "#   - citations list (source + chunk_id)\n",
    "#   - raw_chunks (original chunk text)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_and_deduplicate(chunks: list) -> list:\n",
    "    \"\"\"\n",
    "    Deduplicate chunks by removing near-identical text blocks.\n",
    "    Basic but effective dedup for RAG pipelines.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    for text in chunks:\n",
    "        key = re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_chunks.append(text)\n",
    "    \n",
    "    return unique_chunks\n",
    "\n",
    "\n",
    "def assemble_context(reranked_results: list, max_chars: int = 8000):\n",
    "    \"\"\"\n",
    "    Merge reranked chunks into a final LLM-ready context block.\n",
    "    Includes citation metadata.\n",
    "    \"\"\"\n",
    "    # Extract text chunks\n",
    "    raw_chunks = [item[\"text\"] for item in reranked_results]\n",
    "    \n",
    "    # Deduplicate\n",
    "    unique_chunks = clean_and_deduplicate(raw_chunks)\n",
    "    \n",
    "    # Build the merged context (bounded by max_chars)\n",
    "    merged_text = \"\"\n",
    "    citations = []\n",
    "    \n",
    "    for item, text in zip(reranked_results, unique_chunks):\n",
    "        \n",
    "        if len(merged_text) + len(text) > max_chars:\n",
    "            break\n",
    "        \n",
    "        merged_text += text.strip() + \"\\n\\n---\\n\\n\"\n",
    "        \n",
    "        citations.append({\n",
    "            \"source\": item[\"source\"],\n",
    "            \"chunk_preview\": text[:120] + \"...\",\n",
    "            \"rerank_score\": item.get(\"rerank_score\", None)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"merged_context\": merged_text.strip(),\n",
    "        \"citations\": citations,\n",
    "        \"raw_chunks\": unique_chunks\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ðŸ” Try it with your reranked results\n",
    "# ---------------------------\n",
    "context_for_llm = assemble_context(reranked, max_chars=8000)\n",
    "\n",
    "print(\"ðŸ“¦ Final Context Block Ready\\n\")\n",
    "print(context_for_llm[\"merged_context\"][:800])  # preview first 800 chars\n",
    "print(\"\\nðŸ”— Citations:\")\n",
    "for c in context_for_llm[\"citations\"]:\n",
    "    print(f\"- {c['source']} | {c['chunk_preview']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30f4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_for_llm exists: True\n",
      "dict_keys(['merged_context', 'citations', 'raw_chunks'])\n"
     ]
    }
   ],
   "source": [
    "print(\"context_for_llm exists:\", 'context_for_llm' in locals())\n",
    "\n",
    "if 'context_for_llm' in locals():\n",
    "    print(context_for_llm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0c459-a13f-4d0c-9c51-b1b2a701e7d6",
   "metadata": {},
   "source": [
    "# LLM Answering Pipeline\n",
    "This section is responsible for:\n",
    "\n",
    "* Running a local, open-source LLM\n",
    "* Constructing the final prompt (query + context)\n",
    "* Generating an answer\n",
    "* Attaching citations\n",
    "* Returning a polished response\n",
    "\n",
    "Implementing open-source software, we will use:\n",
    "\n",
    "Ollama (best, simplest, FREE local LLM runner)\n",
    "\n",
    "Model: llama3 (this can changed eventually in order to improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6794d052-d840-4702-8e9a-aa28926e5b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback HTTP Ollama client (requests).\n"
     ]
    }
   ],
   "source": [
    "# Imports + Global Settings\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Optional: pretty printing\n",
    "import textwrap\n",
    "\n",
    "# Load Ollama local LLM client (try the official package first, fall back to HTTP)\n",
    "# Preferred: `pip install ollama`\n",
    "llm = None\n",
    "try:\n",
    "    from ollama import Client as OllamaClient\n",
    "    llm = OllamaClient(host=\"http://localhost:11434\")\n",
    "    print(\"Local LLM client ready (ollama package).\")\n",
    "except Exception as e:\n",
    "    # Fallback: lightweight HTTP client using requests\n",
    "    try:\n",
    "        import requests\n",
    "        from requests.exceptions import HTTPError, RequestException\n",
    "\n",
    "        class SimpleOllamaClient:\n",
    "            def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "                self.host = host.rstrip(\"/\")\n",
    "\n",
    "            def _normalize(self, data):\n",
    "                # Normalize common response shapes to a consistent dict\n",
    "                # Ollama's primary shape: {\"response\": \"...\", \"model\": \"...\", ...}\n",
    "                if isinstance(data, dict):\n",
    "                    # Check for Ollama's native \"response\" key first\n",
    "                    if \"response\" in data:\n",
    "                        return {\"message\": {\"content\": data[\"response\"]}}\n",
    "                    # Check for standard message.content shape\n",
    "                    if \"message\" in data and isinstance(data[\"message\"], dict) and \"content\" in data[\"message\"]:\n",
    "                        return data\n",
    "                    # Check for simple text key\n",
    "                    if \"text\" in data:\n",
    "                        return {\"message\": {\"content\": data[\"text\"]}}\n",
    "                    # Check for choices array (OpenAI-like)\n",
    "                    if \"choices\" in data and isinstance(data[\"choices\"], list) and len(data[\"choices\"]) > 0:\n",
    "                        first = data[\"choices\"][0]\n",
    "                        if isinstance(first, dict) and (\"text\" in first or \"message\" in first):\n",
    "                            text = first.get(\"text\") or (first.get(\"message\") if isinstance(first.get(\"message\"), str) else (first.get(\"message\", {}).get(\"content\") if isinstance(first.get(\"message\"), dict) else None))\n",
    "                            return {\"message\": {\"content\": text}}\n",
    "                # Fallback: stringify\n",
    "                return {\"message\": {\"content\": str(data)}}\n",
    "\n",
    "            def chat(self, model: str, messages: list):\n",
    "                \"\"\"Send a consolidated prompt to Ollama HTTP API and return a normalized response.\n",
    "\n",
    "                Returns a dict containing `{\"message\": {\"content\": <str>}}` on success.\n",
    "                \"\"\"\n",
    "                prompt_text = \"\\n\".join([m.get(\"content\", \"\") for m in messages if m.get(\"role\") == \"user\"]).strip()\n",
    "\n",
    "                url = f\"{self.host}/api/generate\"\n",
    "                payload = {\"model\": model, \"prompt\": prompt_text, \"stream\": False}\n",
    "\n",
    "                try:\n",
    "                    resp = requests.post(url, json=payload, timeout=60)\n",
    "                    resp.raise_for_status()\n",
    "                except HTTPError as he:\n",
    "                    status = getattr(he.response, \"status_code\", None)\n",
    "                    if status == 404:\n",
    "                        raise RuntimeError(\n",
    "                            f\"Ollama HTTP endpoint not found (404). Is the Ollama daemon running at {self.host}?\"\n",
    "                        ) from he\n",
    "                    raise\n",
    "                except RequestException as re:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Failed to reach Ollama at {self.host}: {re}. Ensure the daemon is running and reachable.\"\n",
    "                    ) from re\n",
    "\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except ValueError:\n",
    "                    # Non-JSON response\n",
    "                    return {\"message\": {\"content\": resp.text}}\n",
    "\n",
    "                return self._normalize(data)\n",
    "\n",
    "        llm = SimpleOllamaClient(host=\"http://localhost:11434\")\n",
    "        print(\"Using fallback HTTP Ollama client (requests).\")\n",
    "    except Exception as e2:\n",
    "        print(\"Could not import `ollama` package or use HTTP fallback.\")\n",
    "        print(\"To enable the local LLM client, either:\")\n",
    "        print(\"  1) pip install ollama\")\n",
    "        print(\"  2) ensure Ollama daemon is running at http://localhost:11434 and install requests (`pip install requests`)\")\n",
    "        print(\"Falling back to a stub llm object that raises if used.\")\n",
    "\n",
    "        class _StubLLM:\n",
    "            def chat(self, *args, **kwargs):\n",
    "                raise RuntimeError(\"No Ollama client available. Install `ollama` or `requests` and start the Ollama daemon.\")\n",
    "\n",
    "        llm = _StubLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f7e887-7abe-4c11-9d50-efd02b9df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# ðŸ“˜ Notebook 04 â€” Cell 2\n",
    "# Prompt Template + Formatting Function\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def build_rag_prompt(query: str, context_data: Dict):\n",
    "    \"\"\"\n",
    "    Build the final RAG prompt to send to the local LLM.\n",
    "    Includes:\n",
    "    - merged context\n",
    "    - user question\n",
    "    - instruction to use citations\n",
    "    \"\"\"\n",
    "\n",
    "    context_block = context_data[\"merged_context\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant answering questions using ONLY the context provided.\n",
    "\n",
    "CONTEXT:\n",
    "-------\n",
    "{context_block}\n",
    "-------\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use ONLY the information in the context.\n",
    "- Do NOT hallucinate.\n",
    "- Cite your sources using this format: [source].\n",
    "- If the answer is not in the context, say: \"The answer is not available in the provided context.\"\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "FINAL ANSWER (with citations):\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e302831-91a0-4a6d-9f26-086f3ae51488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LLM ANSWER =====\n",
      "\n",
      "According to the context, 6G offers the unification of experiences across the digital, physical, and human worlds [1], as well as a bouquet of unique expectations that redefine how we live and protect our planet. Additionally, it is expected to support Internet-of-Everything (IoE) services and radical advancements in human-machine interaction technologies [1].\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# ðŸ“˜ Notebook 04 â€” Cell 3\n",
    "# Generate Answer using Local LLM (Ollama)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def generate_llm_answer(query: str, context_for_llm: Dict, model: str = None):\n",
    "    \"\"\"\n",
    "    Sends the final prompt to the local LLM and retrieves the response.\n",
    "    If model is not specified, uses OLLAMA_MODEL env var (default: llama3.1:latest).\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:latest\")\n",
    "    \n",
    "    prompt = build_rag_prompt(query, context_for_llm)\n",
    "\n",
    "    try:\n",
    "        response = llm.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        # Re-raise runtime errors from the client (e.g., helpful 404 message)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\") from e\n",
    "\n",
    "    # Attempt to extract a text response from a range of possible shapes\n",
    "    content = None\n",
    "    if isinstance(response, dict):\n",
    "        # Check for normalized message.content first (most common after _normalize)\n",
    "        if \"message\" in response and isinstance(response[\"message\"], dict) and \"content\" in response[\"message\"]:\n",
    "            content = response[\"message\"][\"content\"]\n",
    "        # Fallback checks in case _normalize wasn't applied\n",
    "        elif \"response\" in response:\n",
    "            content = response[\"response\"]\n",
    "        elif \"text\" in response:\n",
    "            content = response[\"text\"]\n",
    "        elif \"choices\" in response and isinstance(response[\"choices\"], list) and len(response[\"choices\"]) > 0:\n",
    "            first = response[\"choices\"][0]\n",
    "            if isinstance(first, dict):\n",
    "                content = first.get(\"text\") or (first.get(\"message\") if isinstance(first.get(\"message\"), str) else (first.get(\"message\", {}).get(\"content\") if isinstance(first.get(\"message\"), dict) else None))\n",
    "\n",
    "    if content is None:\n",
    "        # Fallback: stringify whatever we got\n",
    "        content = str(response)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# ---- Test the answering pipeline (only if variables are defined) ----\n",
    "# If 'context_for_llm' comes from notebook 03, use it. Otherwise, use a sample query.\n",
    "if 'context_for_llm' in locals():\n",
    "    # Use the query and context from notebook 03\n",
    "    sample_answer = generate_llm_answer(query, context_for_llm)\n",
    "    \n",
    "    print(\"\\n===== LLM ANSWER =====\\n\")\n",
    "    print(sample_answer)\n",
    "else:\n",
    "    print(\"âš ï¸ Variable 'context_for_llm' not found in kernel.\")\n",
    "    print(\"Please run Notebook 03 first to generate the retrieval context, then return here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703fd922-c0f3-46b2-890a-1d2431beb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================\n",
      "FINAL ANSWER\n",
      "=======================\n",
      "\n",
      "According to the context, 6G offers the unification of experiences across the digital, physical, and human worlds [1], as well as a bouquet of unique expectations that redefine how we live and protect our planet. Additionally, it is expected to support Internet-of-Everything (IoE) services and radical advancements in human-machine interaction technologies [1].\n",
      "\n",
      "=======================\n",
      "CITATIONS\n",
      "=======================\n",
      "\n",
      "- Source: ../data/test.pdf\n",
      "  Preview: The Journey A Digital and Societal Lina Mohjazi, Bassant Selim, Mallik Abstractâ€”While the fifth generation (5 G) is brin...\n",
      "  Score: 0.0978\n",
      "\n",
      "- Source: ../data/test.pdf\n",
      "  Preview: anticipated 6 G promise into reality. REFERENCES [1] M. Al-Quraan, L. Mohjazi, L. Bariah, A. Centeno, A. Zoha, K. Arshad...\n",
      "  Score: -1.5797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# ðŸ“˜ Notebook 04 â€” Cell 4\n",
    "# Final Output Formatting (Answer + Citations)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def format_final_output(answer: str, context_for_llm: Dict):\n",
    "    \"\"\"\n",
    "    Returns a clean, structured output including:\n",
    "    - Answer\n",
    "    - Citations\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(\"=======================\\n\")\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n=======================\")\n",
    "    print(\"CITATIONS\")\n",
    "    print(\"=======================\\n\")\n",
    "    for c in context_for_llm[\"citations\"]:\n",
    "        print(f\"- Source: {c['source']}\")\n",
    "        print(f\"  Preview: {c['chunk_preview']}\")\n",
    "        print(f\"  Score: {c['rerank_score']:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Display final result (only if variables are defined)\n",
    "if 'sample_answer' in locals() and 'context_for_llm' in locals():\n",
    "    format_final_output(sample_answer, context_for_llm)\n",
    "else:\n",
    "    print(\"âš ï¸ Variables 'sample_answer' and/or 'context_for_llm' not found.\")\n",
    "    print(\"Please run cells above first to generate these variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6d684-4dd3-4867-98ed-bc5046389490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76cd2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ollama HTTP Diagnostic ---\n",
      "Using host=http://localhost:11434 model=llama3.1:latest\n",
      "requests version: 2.32.5\n",
      "GET  -> 200\n",
      "GET /api -> 404\n",
      "GET /api/models -> 404\n",
      "GET /api/generate -> 405\n",
      "POST /api/generate -> 200\n",
      "Response (truncated): {\"model\":\"llama3.1:latest\",\"created_at\":\"2025-11-15T20:04:34.737354869Z\",\"response\":\"Hello! I'm a large language model, specifically a variant of the BERT (Bidirectional Encoder Representations from Transformers) architecture, fine-tuned on a variety of tasks to generate human-like responses.\\n\\nMy specific model is based on the T5 (Text-To-Text Transfer Transformer) framework, which allows me to understand and respond to a wide range of natural language inputs. This combination of BERT and T5 enables me to provide helpful and accurate answers to your questions!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,46864,24748,323,10765,279,1646,1511,13,128009,128006,78191,128007,271,9906,0,358,2846,264,3544,4221,1646,11,11951,264,11678,315,279,426,3481,320,66552,45770,56215,2\n",
      "JSON keys: ['model', 'created_at', 'response', 'done', 'done_reason', 'context', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration']\n",
      "Normalized sample: {'message': {'content': \"Hello! I'm a large language model, specifically a variant of the BERT (Bidirectional Encoder Representations from Transformers) architecture, fine-tuned on a variety of tasks to generate human-like responses.\\n\\nMy specific model is based on the T5 (Text-To-Text Transfer Transformer) framework, which allows me to understand and respond to a wide range of natural language inputs. This combination of BERT and T5 enables me to provide helpful and accurate answers to your questions!\"}}\n",
      "Diagnostic complete. If you see 404 from POST, verify the Ollama daemon and model name (try OLLAMA_MODEL=llama3.1:latest).\n"
     ]
    }
   ],
   "source": [
    "# ---- Ollama HTTP diagnostic (run this cell to test your local Ollama docker)\n",
    "import os\n",
    "print(\"--- Ollama HTTP Diagnostic ---\")\n",
    "host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "model = os.getenv('OLLAMA_MODEL', 'llama3.1:latest')\n",
    "print(f'Using host={host} model={model}')\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print('requests version:', requests.__version__)\n",
    "except Exception as e:\n",
    "    print('requests not available in this kernel:', e)\n",
    "\n",
    "# Quick GET checks for common endpoints\n",
    "for path in ['', '/api', '/api/models', '/api/generate']:\n",
    "    url = host.rstrip('/') + path\n",
    "    try:\n",
    "        r = requests.get(url, timeout=4)\n",
    "        print(f'GET {path} ->', r.status_code)\n",
    "    except Exception as e:\n",
    "        print(f'GET {path} error:', e)\n",
    "\n",
    "# Try a small generate POST to /api/generate\n",
    "payload = {'model': model, 'prompt': 'Say hello and identify the model used.', 'stream': False}\n",
    "try:\n",
    "    gen_url = host.rstrip('/') + '/api/generate'\n",
    "    r = requests.post(gen_url, json=payload, timeout=20)\n",
    "    print('POST /api/generate ->', r.status_code)\n",
    "    text = r.text\n",
    "    print('Response (truncated):', text[:800])\n",
    "    try:\n",
    "        j = r.json()\n",
    "        print('JSON keys:', list(j.keys()))\n",
    "        # If llm object exists with _normalize, try to normalize and show result\n",
    "        if 'llm' in globals() and hasattr(llm, '_normalize'):\n",
    "            try:\n",
    "                print('Normalized sample:', llm._normalize(j))\n",
    "            except Exception as e:\n",
    "                print('Normalization error:', e)\n",
    "        else:\n",
    "            # Try simple normalization heuristics\n",
    "            if isinstance(j, dict):\n",
    "                if 'message' in j:\n",
    "                    print('message:', j.get('message'))\n",
    "                elif 'text' in j:\n",
    "                    print('text:', j.get('text'))\n",
    "                elif 'choices' in j and isinstance(j['choices'], list) and len(j['choices'])>0:\n",
    "                    print('choices[0]:', j['choices'][0])\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('POST /api/generate error:', e)\n",
    "\n",
    "print('Diagnostic complete. If you see 404 from POST, verify the Ollama daemon and model name (try OLLAMA_MODEL=llama3.1:latest).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23f487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
