{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b3ffb0",
   "metadata": {},
   "source": [
    "# 1 Setup & Library Imports\n",
    "\n",
    "First, we import the necessary libraries and\n",
    "set up the directory where our ingested and processed data will live.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ad2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory ready at: ./data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pdfplumber  # For extracting text from PDF documents\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Directory configuration \n",
    "# We'll store raw files (PDFs, CSVs, etc.) inside a \"data\" folder.\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# Define the path where we'll save processed text chunks.\n",
    "# JSONL (JSON Lines) format is used because it handles large datasets efficiently:\n",
    "# each line is a valid JSON object, so you can read it line by line.\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"chunks.jsonl\")\n",
    "\n",
    "# Create the directory if it doesn't exist.\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory ready at: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfd00be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: /home/diego/Projects/advanced-multisource-rag-project/data\n",
      "\n",
      "Files found:\n",
      "- MOCK_DATA.csv:Zone.Identifier\n",
      "- test.pdf\n",
      "- .ipynb_checkpoints\n",
      "- The Journey Towards 6G - A Digital and Societal Revolution in the Making - IEEE Resource - copia.pdf:Zone.Identifier\n",
      "- chunks.jsonl\n",
      "- MOCK_DATA.csv\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "data_path = os.path.abspath('./data')\n",
    "print(f\"Looking for files in: {data_path}\")\n",
    "files = os.listdir('./data')\n",
    "print(\"\\nFiles found:\")\n",
    "for file in files:\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a478",
   "metadata": {},
   "source": [
    "# 2 Load and Extract Text from PDFs\n",
    "\n",
    "This function will scan the data folder for any PDF files\n",
    "and extract their text content using the pdfplumber library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57365011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PDF extraction (this may take a moment for large files)...\n",
      "\n",
      "Processing ./data/test.pdf (timeout=60s)...\n",
      "   Found 9 pages. Extracting text...\n",
      "Successfully extracted text from ./data/test.pdf\n",
      "\n",
      "Loaded 1 PDFs with column-aware extraction.\n",
      "\n",
      "Extracted 1 PDF documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from typing import List, Dict\n",
    "import signal\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    \"\"\"Handler for timeout signal.\"\"\"\n",
    "    raise TimeoutError(\"PDF extraction timed out\")\n",
    "\n",
    "def load_pdfs_column_aware(pdf_paths: List[str] | str, n_columns: int = 2, timeout_sec: int = 60) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extracts text from multi-column PDFs, processing each column separately.\n",
    "    Includes a timeout mechanism to prevent hanging on large/problematic PDFs.\n",
    "\n",
    "    Args:\n",
    "        pdf_paths (List[str] | str): List of PDF file paths, a single PDF path,\n",
    "            or a directory path containing PDF files.\n",
    "        n_columns (int): Number of columns per page (default=2).\n",
    "        timeout_sec (int): Timeout in seconds per PDF (default=60).\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of dictionaries with \"id\", \"source\", and \"text\".\n",
    "    \"\"\"\n",
    "    # Normalize input: accept a single path string, a list of paths, or a directory\n",
    "    if isinstance(pdf_paths, str):\n",
    "        if os.path.isdir(pdf_paths):\n",
    "            # Directory: collect .pdf files inside\n",
    "            pdf_list = [os.path.join(pdf_paths, f) for f in os.listdir(pdf_paths) if f.lower().endswith('.pdf')]\n",
    "        else:\n",
    "            pdf_list = [pdf_paths]\n",
    "    else:\n",
    "        # assume iterable of paths\n",
    "        pdf_list = list(pdf_paths)\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for path in pdf_list:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Skipping missing path: {path}\")\n",
    "            continue\n",
    "        if os.path.isdir(path):\n",
    "            # Skip directories (already handled), but be defensive\n",
    "            print(f\"Skipping directory path: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {path} (timeout={timeout_sec}s)...\")\n",
    "        all_text = []\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(path) as pdf:\n",
    "                num_pages = len(pdf.pages)\n",
    "                print(f\"   Found {num_pages} pages. Extracting text...\")\n",
    "                \n",
    "                for page_idx, page in enumerate(pdf.pages):\n",
    "                    # Print progress every 10 pages\n",
    "                    if (page_idx + 1) % 10 == 0:\n",
    "                        print(f\"   Progress: {page_idx + 1}/{num_pages} pages...\")\n",
    "                    \n",
    "                    width = page.width\n",
    "                    height = page.height\n",
    "\n",
    "                    # Split the page width into N equal column boxes\n",
    "                    column_width = width / n_columns\n",
    "                    for i in range(n_columns):\n",
    "                        left = i * column_width\n",
    "                        right = (i + 1) * column_width\n",
    "                        bbox = (left, 0, right, height)\n",
    "                        column = page.within_bbox(bbox)\n",
    "                        try:\n",
    "                            text = column.extract_text(x_tolerance=2, y_tolerance=2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Column {i} extraction failed on page {page_idx + 1}: {e}\")\n",
    "                            text = None\n",
    "                        if text:\n",
    "                            all_text.append(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if all_text:\n",
    "            combined_text = \"\\n\".join(all_text)\n",
    "            docs.append({\n",
    "                \"id\": os.path.basename(path).replace(\".pdf\", \"\"),\n",
    "                \"source\": path,\n",
    "                \"text\": combined_text\n",
    "            })\n",
    "            print(f\"Successfully extracted text from {path}\")\n",
    "        else:\n",
    "            print(f\"No text extracted from {path}\")\n",
    "\n",
    "    print(f\"\\nLoaded {len(docs)} PDFs with column-aware extraction.\")\n",
    "    return docs\n",
    "\n",
    "# Example use: accept a single path, a list, or a directory\n",
    "print(\"Starting PDF extraction (this may take a moment for large files)...\\n\")\n",
    "pdf_docs = load_pdfs_column_aware(DATA_DIR)\n",
    "print(f\"\\nExtracted {len(pdf_docs)} PDF documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba5047",
   "metadata": {},
   "source": [
    "# 3 Load and Extract Text from CSV Files\n",
    "Some data might come in structured tabular form (CSV).\n",
    "We'll read them using pandas and convert each row into a text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13aab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: MOCK_DATA.csv\n",
      "Loaded 1000 CSV row(s).\n"
     ]
    }
   ],
   "source": [
    "def load_csvs(data_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and flatten CSV files into text chunks.\n",
    "    Each row becomes a small text paragraph joined by commas.\n",
    "    \"\"\"\n",
    "    csv_data = []\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.lower().endswith(\".csv\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            print(f\"Reading CSV: {file}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Convert each row into a textual form (for embedding later)\n",
    "                for _, row in df.iterrows():\n",
    "                    row_text = \", \".join([f\"{col}: {str(row[col])}\" for col in df.columns])\n",
    "                    csv_data.append({\n",
    "                        \"source\": file,\n",
    "                        \"text\": row_text,\n",
    "                        \"type\": \"csv\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    return csv_data\n",
    "\n",
    "\n",
    "# Run and preview the CSV loading function\n",
    "csv_docs = load_csvs(DATA_DIR)\n",
    "print(f\"Loaded {len(csv_docs)} CSV row(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bcc54",
   "metadata": {},
   "source": [
    "# 4 Combine and Clean Documents\n",
    "\n",
    "Merge all documents (PDFs + CSVs) into one list.\n",
    "We'll also do light cleaning — remove empty or very short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3531b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 1001 documents after cleaning.\n"
     ]
    }
   ],
   "source": [
    "all_docs = pdf_docs + csv_docs\n",
    "\n",
    "# Filter out empty entries\n",
    "all_docs = [doc for doc in all_docs if len(doc[\"text\"].strip()) > 50]\n",
    "\n",
    "print(f\"Combined {len(all_docs)} documents after cleaning.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de338045",
   "metadata": {},
   "source": [
    "# 4.1 Clean and Normalization of Extracted Text\n",
    "In later steps issues were found with how pdfplumber extracts text and preserves some elements from the original document such as line breaks and hyphens. \n",
    "This next cell will normalize text before chunking in order to improve readability and retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e15ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied advanced cleaning to 1001 documents.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_advanced(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and normalizes extracted text from PDFs/CSVs, with better spacing control.\n",
    "    - Fixes missing spaces after punctuation or between words merged by PDF parsing.\n",
    "    - Removes excessive newlines and spaces.\n",
    "    - Keeps acronyms and numbers readable.\n",
    "    \"\"\"\n",
    "    # Replace newlines and tabs with spaces\n",
    "    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "\n",
    "    # Add a space between letters and numbers if merged (e.g., \"5Gnetwork\" -> \"5G network\")\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)\n",
    "    text = re.sub(r'(?<=\\d)(?=[a-zA-Z])', ' ', text)\n",
    "\n",
    "    # Add missing space between lowercase-uppercase transitions (e.g., “inB5G” -> “in B5G”)\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "\n",
    "    # Fix hyphenated line breaks (e.g., \"inter-\\nnational\" -> \"international\")\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Trim spaces at start and end\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply improved cleaning to all documents\n",
    "for doc in all_docs:\n",
    "    doc[\"text\"] = clean_text_advanced(doc[\"text\"])\n",
    "\n",
    "print(f\"Applied advanced cleaning to {len(all_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6f306",
   "metadata": {},
   "source": [
    "# 5 Chunk the Text\n",
    "\n",
    "We split large documents into smaller pieces (\"chunks\").\n",
    "This is critical for RAG systems since embeddings work best\n",
    "on 200–1000 token segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c1c9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1073 sentence-aware chunks from 1001 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/diego/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Notebook 01 — Improved Chunking (Recommended for RAG)\n",
    "# Using newer langchain import path (v0.1.0+)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def smart_chunk_text(text: str):\n",
    "    \"\"\"\n",
    "    Improved chunking:\n",
    "    - Sentence-aware\n",
    "    - Recursive splits for edge cases\n",
    "    - Multi-level separators\n",
    "    - Maintains consistent ~800-token chunks with overlap\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,      # target chunk length in characters (≈ tokens)\n",
    "        chunk_overlap=150,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \". \",\n",
    "            \"? \",\n",
    "            \"! \",\n",
    "            \"; \",\n",
    "            \", \",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Apply to all documents (keeps original logic)\n",
    "chunked_docs = []\n",
    "for doc in all_docs:\n",
    "    chunks = smart_chunk_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_docs.append({\n",
    "            \"id\": f\"{doc.get('id', i)}_chunk{i}\",\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} sentence-aware chunks from {len(all_docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead7122",
   "metadata": {},
   "source": [
    "# 6 Save Chunks to JSONL File\n",
    "We save the processed chunks so that future steps (embedding generation,\n",
    "indexing, retrieval) can load them efficiently without re-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbd748c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1073 chunks to ./data/chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in chunked_docs:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(chunked_docs)} chunks to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea157090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
