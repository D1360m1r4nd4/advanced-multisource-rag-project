{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d55201",
   "metadata": {},
   "source": [
    "# Chunking in RAG systems\n",
    "\n",
    "## 1 Embeddings Context Limits\n",
    "\n",
    "When embedings (vector representation of text) are created, most models have a maximum token limit (usually around 1000 tokens -> 750 words). \n",
    "\n",
    "If text exceeds that limit: \n",
    "* The model either truncates the input (loosing information in the process).\n",
    "* You can't process it at all.\n",
    "So splitting ensures every piece fits within the range of what the embedding model can handle.\n",
    "\n",
    "## 2 Better Semantic Accuracy\n",
    "\n",
    "Embeddings capture the meaning of a text chunk as a single vector in high-dimensional space.\n",
    "\n",
    "If a chunk is too large:\n",
    "\n",
    "* It mixes multiple topics or sections together.\n",
    "* The embedding becomes less precise - it represents an average of different ideas.\n",
    "\n",
    "If a chung is too small:\n",
    "* It may lack context and produce embeddings that are too narrow or meaningless.\n",
    "\n",
    "That's why most RAG systems target the range between 200-1000 tokens.\n",
    "\n",
    "## 3 Efficient Retrieval\n",
    "\n",
    "When a user aks a question in a RAG system: \n",
    "\n",
    "1. The system converts the question into an embedding vector. \n",
    "2. It retrieves the most similar document chunk (based on cosine similarity). \n",
    "3. It sends those relevant chunks to the LLM to generate the answer.\n",
    "\n",
    "If you don't chunk: \n",
    "* You'd have to embed massive documents - which is slow and expensive.\n",
    "* Retrieval would be less focused - the model might pull irrelevant sections.\n",
    "\n",
    "## 4 Easier Updates and Maintenance\n",
    "Smaller chunks mean:\n",
    "\n",
    "* You can re-embed or update only affected sections (not the entire document).\n",
    "* You can parallelize embedding jobs easily.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
